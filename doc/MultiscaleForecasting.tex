\documentclass[12pt]{article}
\usepackage{a4wide}
\usepackage{NotationStyle}
\usepackage[]{algorithm2e}
%\usepackage[noend]{algorithmic}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,mathrsfs,mathtext}
\usepackage{subfig}
\usepackage{graphics,graphicx,epsfig}
\usepackage{epstopdf}
\usepackage{fancybox,fancyhdr}
\usepackage{enumerate}
\usepackage{array}
\usepackage{color, soul}
\usepackage{longtable}
\usepackage[normalem]{ulem}
\usepackage{arydshln}
\setlength\dashlinedash{0.2pt}
\setlength\dashlinegap{4.5pt}

\newcommand{\argmax}{\mathop{\rm arg\,max}\limits}
\graphicspath{ {fig/} }

\renewcommand{\baselinestretch}{1.3}
\begin{document}
\title{Feature generation for multiscale time series forecasting multimodels}
\author{LIG}%Radoslav Neychev, Eric Gaussier and Vadim Strijov}
\date{Technical report (pre-draft)}
\maketitle

\section*{Notation}
\begin{longtable}{p{4.2cm}p{12cm}}
$\fD=\{\bs^{(m)}\}$, \par $m = 1, \dots, M$ & large set of time series \\
$\bar{\bs}$ & special case of time series $\bs$ for classification \\
\textcolor{red}{$T_{\max}$} & maximum number of time stamps \\
$\dtp$ & local prehistory \\
$\dtr$ & defines time period (requested) for prediction \\
$\bs^{(m)}_i = [\y^{(m)}_i|\x^{(m)}_i]$ & local ($t_i$) segment from the time series $\bs^{(m)}$ \\
$\bX^{*}$, $\bx_i^{*} = [\y_i|\x_i]$ & object-feature matrix, $\by_i = [\y^{1}_i, \dots, \y^{M}_i]$, $\x_i = [\x^{1}_i, \dots, \x^{M}_i]$ \\
$\bY$, $\bX$ & target matrix, feature matrix \\
\textcolor{red}{$q$} & number of objects $\bx_i^{*}$ extracted from $\fD$ \\
$r$ & dimensionality of the target vector $[\y_i, \y'_i, \dots]$ \\
$n$ & dimensionality of the feature vector $[\x_i, \x'_i, \dots]$ \\
$\bx_0^{*} = [\hat{y} |\bx_0]$ & a testing object? \\
$\cJ = {1,\dots, n}$ & feature index set \\
$\cA \subset \cJ$ & the subset of feature indices \\
$\fx_k(\w_{\cA_k}, \x)$, $k=1,\dots,K$ & regression model, defined for a subset $\cA_k$ \\
$\w_{\cA_k}$ & model parameters (alternatively, $\w_k$)\\
$\boldsymbol{\pi} = [\pi_{i1}, \dots, \pi_{iK}]$ & vector of correspondence \\
$\cI = \{1, \dots, m\} = \cB_0 \sqcup_k \cB_k$ & set of object indices \\
$\cB_0$ & test set \\
$\cB_k$, $k=1,\dots,K$ & training sets \\
$Q$ & quality function \\
$S$ & error function \\
$g=g(\bb,s)$ & parametric functions for feature transformation \\
$\mathfrak{G} = \{g:\bs\to\vf\}$ & set of feature transformations \\
$\vf=[\vf^1,\dots,\vf^{M}]$ & generated feature vector \\
$\veps = \by - \hat{\by}$ & vector of residuals \\
$\bc^1_1,\dots,\bc^M_P$ & $k$-means centroids \\
$\mathbf{H}^{*}$ & Hankel matrix \\
$\bth = [\w_1, \dots, \w_k, \boldsymbol{\pi}, \beta]$ & parameters of the mixture model \\
$z_{ik}$, $\gamma_{ik}$ & latent (correspondence) variables and their expectations (mixture models) \\
\end{longtable}

\section{Introduction}
The paper investigates behavior of a device within the concept of Internet of Things. The device at question is monitored by a set of sensors, which produces large amount of multiscale time series during its lifespan. These time series have various time scales since distinct sensor produce measurements with various frequencies (milliseconds, days, weeks, etc).  The main goal is to forecast the next state of a device. \hl{What exactly is this state?}

We assume that the sampling rate of each time series is fixed and each time series has its own forecast horizon. Examples of this kind of time series are listed below. \hl{(list)}

Furthermore, the time history is assumed sufficiently long to construct an adequate forecasting model; the time series are assumed to have auto- and cross-correlation dependencies. 

\hl{The model is static, so there exists a history of optimal size. Each time series could be interpolated by some local model (piece-wise constant, piece-wise linear), and that there exist a local approximation model, which could be applied in the case of local data absence.}

In the next Sections we explain how to construct \hl{vector autoregressive models} and test their quality under the listed conditions.
 TODO (list and review state of the art).

\section{Problem statement}
Consider a large set of time series~$\fD=\{\bs^{(m)}| \; m = 1\dots, {M}\}$, where each real-valued time series~$\bs$  has its own sampling rate $f^{(m)}_s$. The  sampling rate might be changing over time, or the ratio between different sampling rates might be non-rational. In such cases as well as for time series with missing values we apply resampling procedure, which is described further.

\subsection{Time series resampling}
%\hl{Assuming each time series could be interpolated introduce a joint sample rate and assign each series~$\bs$ has minimum one and maximum~$T_{\max}$ samples for one time-tick of this rate. Introduce joint time-scale and split it to the following parts, as Fig.~\ref{fig_resample1} shows.
%For the time series of sample rate that is changing, unstable, non-rational to the common rate as well as for the time series with missing values the following procedure should be applied.}

Let the time~$t$ be in continuous set~$\mathbb{R}_+^1$ and the time series~$\bs$ be piece-wise constant. There are three possibilities to create such time series from a discrete-values one: 1) the constant goes after the sample~$s(t)$, 2) before the sample, 3) in the neighborhood of the sample. See red, green and blue lines in the Figure~\ref{fig_resample1}.

\begin{figure}[!ht]
\centering\includegraphics[width=0.5\textwidth]{resample1.png}
\caption{Piece-wise representation of a time series.}\label{fig_resample1}
\end{figure}

This assumptions helps introducing a new sampling rate and eliminates the problem of missing values, since the previous (next, current in the terms of Fig.~\ref{fig_resample1}) value holds continuously until the following comes. The constant model could be developed into more complex one: a piece-wise linear, quadratic or cubic spline with its nodes in the time-ticks or over the time-ticks according to the following criterions: 1) Nyquist됩hannon theorem, 2) Fisher-Neyman theorem. The following optimization problem returns the new sampling rate:
\[
\text{todo}
\]
This fixed rate is used to obtain a resampled time series with regular time-ticks.

\begin{figure}[!ht]
\centering\includegraphics[width=0.9\textwidth]{resample2.png}
\caption{Resample time series of variable sample rate into the fixed one with the optimal period.}\label{fig_resample2}
\end{figure}


\paragraph{Selecting sampling rate: Nyquist됩hannon  sampling theorem.}
Suppose that a signal $s(t)$ is bandlimited with frequency $f_b$. According to the Nyquist됩hannon  sampling theorem, it is sufficient to sample the signal with frequency \begin{equation}\label{eq:nyquist_condition}
\frac{1}{T} = f_s > 2f_b\end{equation} 
to be able to fully reconstruct the signal from its discretely sampled measurements $s(t_i) = s(iT)$. Alternatively, for $f_s > 2f_b$, Whittaker됩hannon interpolation
\[s(t) = \sum_{i = -\infty}^{\infty} s(i\cdot T)\text{sinc}\left(\frac{t-iT}{T}\right)\]
of the time series $s(t_i)$ yields perfect reconstruction of the signal $s(t)$. Sampling with $f_s < 2B$ causes distortions known as aliasing. In this case the time series have to be low-pass filtered to satisfy the Nyquist condition~\eqref{eq:nyquist_condition}.

%\paragraph{Selecting sampling rate: power spectrum analysis.}

\paragraph{FFT resampling.}
In signal processing a common way to change resolution of a signal is to use a combination of upsampling and decimation, implemented here via FFT transform and downsampling. Suppose that desired sampling rate $f_{rs}$ is fixed, that is, we would like to approximate $\Trs = f_{rs}\Ts$ uniformly sampled observations ${s}(\tilde{t}_i), \; i = 1, \dots T_{rs}$ of the time series $s(t_i), \; i=1, \dots, \Ts$. Let $\Gs = \{t_1, \dots, \Ts\}$ and $\Grs = \{\tilde{t}_1, \dots, T_{rs}\}$ denote the current and the desired grids.
The first step to resampling $\bs(\Gs) \rightarrow \bs(\Grs)$ is the piecewise approximation $\hat{\bs} = \bs(\Gj)$ (see Fig.~\ref{fig_resample1} and Fig.~\ref{fig_resample2})
\begin{equation}\label{eq:piecewise_c}
 % \hat{s}(\hat{t}_i) = \begin{cases} s(\hat{t}_i), \text{~if~} t_i \in \Gs, \text{~otherwise:} \\
%  s(t_i), \text{~where~} t_i = \max\{t|t\in \Gs ~\text{and~} t < t_i\}.\end{cases}
\hat{s}(\hat{t}_i) = s(t_i), \text{~where~} t_i = \max\{t|t\in \Gs ~\text{and~} t \leq t_i \}.
\end{equation}of $\bs$ at $\Gj = \Gs \cup \Grs$. To increase the smoothness of piecewise approximation $\hat{\bs}$ we apply low pass DFT filtering to $\bs({\Gj})$, so that the reconstructed time series $\bs_{\text{lf}}({\Gj})$ are bandlimited with
$f_{b} < f_{rs}/2$ and then downsample the output $\bs({\Gj})$ to $\Grs$.

\paragraph{Selecting sampling rate: Fisher-Neyman resampling criterion.}
When selecting a new sampling frequency $f_s$ is the local approximation quality of the time series $s(t_i)$, it might also be useful to monitor the variability of $s(t_i)$ as $t_i$ increase. \hl{Not evident how to analyse the variability? If the time series were nearly constant for a period of time and then start changing, it doesn't say much about the sampling rate. Moreover, how  do we measure variability over time while the sampling is still uneven?}



\begin{algorithm}[!ht]
\emph{\textbf{TimeSeriesRescaling()}}

 \KwData{time series $\bs(\Gs)$, sampled at $\Gs = \{t_1, \dots, \Ts\}$. }
 \textbf{Parameters:}{ desired grid $\Grs = \{\tilde{t}_1, \dots \Trs\}$.} \\
 \KwResult{resampled time series $\bs(\Grs) = \{\tilde{s}(\tilde{t}_i)| i\in \Grs \}$.}
 Form the new grid $\Gj = \Gs \cup \Grs$\;
 Upsample time series, using piecewise approximation:

 $\quad \hat{\bs} = \text{~\textbf{Piecewise}}(\bs, \Gs, \Grs)$\;
 Apply low pass filtering $\hat{\bs}$:

 $\quad \hat{\bs}_{\text{lf}} = \text{\textbf{LowPassFFTFiltering}}(\hat{\bs}, \frac{f_{s}}{2} - \varepsilon)$ \;
 Downsample $\hat{\bs}_{\text{lf}}$ to $\tilde{G}$:

 $\quad \tilde{\bs} = [\hat{s}_{lf}(\tilde{t}_1), \dots, ], \; \tilde{t}_i \in \tilde{G}.$
 \bigskip

\emph{\textbf{Piecewise()}}\\
 \KwData{Time series, $\bs$. Parameters: original grid $G$, new grid $\tilde{G}$.}
 \KwResult{Upsampled time series $\hat{\bs} = \{\hat{s}(\hat{t}_i) \in G \cup \tilde{G}\}$.}
 Form new grid $\hat{G} = G \cup \tilde{G}$:
 $ \hat{G} = \{\hat{t}_1 < \dots < \hat{t}_i < \dots < \Ts| \hat{t}_i \in G\cup \tilde{G}\}$ \;
 For each $\hat{t}_i \in \hat{G}: $
 \eIf{$\hat{t}_i \in G$:}{
 $\quad \hat{s}(\hat{t}_i) = x(\hat{t}_i)$ \;}{
 $\quad \hat{s}(\hat{t}_i) = x(t_i)$, where $t_i = \max\{t: t\in G \text{and} t < \hat{t_i}.\}$ }
 \smallskip

 \emph{\textbf{LowPassFFTFiltering()}} \\
 \KwData{Time series, $\bs$. Parameters: cut-off value $f_{b}$ for high frequencies.}
 \KwResult{Filtered time series $\tilde{\bs}_{\text{lf}}$.}
 Zero-pad ${\bs}$, so that $|\bs| = 2^{N}$, where $N = \lceil \log_2(|\bs|)\rceil$\;
 Find FFT coefficients $a_j,\; b_j$ for  $j = 1, \dots, N$ for $\bs$\;
 Set $a_j = 0,\; b_j=0$, for  $2\pi w_j > f_{b}$ \;
 Reconstruct the time series, using inverse FFT.
 \smallskip
 \caption{FFT rescaling procedure.}
\end{algorithm}

\begin{figure}[!ht]
\centering
\subfloat[]{\includegraphics[width=0.8\textwidth]{online_forecasting_paradigm.png}
\label{fg:online_frc}} \\
\centering \subfloat[]{\includegraphics[width=0.8\textwidth]{forecasting_model.png}
\label{fg:forecasting}
}
\caption{Forecasting (a) as regression problem (b).}
\end{figure}

\subsection{Design matrix}
\hl{Assign to the requested forecast time-segment~$\dtr$ a new vector~$\by$ of forecasts. This vector contains forecasts to all time series of the set~$\{\bs\}$. The value of~$\dtr$ is given so that each time series~$\bs$ has minimum one sample. A~proportion of~$T_{\max}$ samples for time series of high frequency sample rates could be considered, too.}

\hl{The first part is the history itself, the second part is the local history, which forms an object of the ``object-feature'' matrix to make a forecast. The third part is a requested forecast time-segment~$\dtr$ and the last one is a \emph{forecast horizon}. Forecast horizon is a time segment~$\Delta t_\text{h}$, which brings an adequate (TODO explain it) forecast~$\bs(\Delta t_\text{h})$ in comparison to the real historical values~$\hat{\bs}(\Delta t_\text{h})$.}

Given the set of properly resampled time series $\fD=\{\bs^{(m)}\}$, $m = 1, \dots, M$, the task is to obtain forecasts $\hat{s}^{(m)}(t_i)$, $\dtr <  t_i \leq T_{\max} + \dtr$ for each time series $\bs^{(m)}$ (Fig.~\ref{fg:online_frc}).

\begin{figure}[!ht]
\centering
\subfloat[]{\includegraphics[width=0.8\textwidth]{draw_object.png}\label{fg:draw}} \\
\centering\subfloat[]{\includegraphics[width=0.8\textwidth]{design_matrix_generation.png}\label{fg:design}}
\caption{Draw an object from time series history.}
\end{figure}

To reformulate the forecasting task into regression task, form an object set at a set of the vectors~$\{\bx^{*}\}$, where each vector~$\bx^{*} = [\by| \bx]$ collects all the time series over the time period~$\dtp$ (Fig.~\ref{fg:draw}), which stands for the local \emph{prehistory}. The vector $\bx^{*}$ could (with no necessity) include samples from previous history of any time series $\bs^{(m)}$ as well as any derivatives $\vf$, which are called generated features.

The ``object-feature'' matrix~$\bX^*$ for the multiscale autoregressive problem statement is constructed  as follows (Fig.~\ref{fg:design}). Let $\bs^{(m)}_i$ denote the~$i$-th segment of the time series $\bs^{(m)}$
\begin{equation}\label{eq:segment}
\bs^{(m)}_i = [\by^{(m)}_i | \bx^{(m)}_i] = [\underbrace{s^{(m)}(t_i),\dots,s^{(m)}(t_i-\dtr)),}_{\by^{(m)}_i}
\underbrace{\dots,s^{(m)}(t_i-\dtr-\dtp)}_{\bx^{(m)}_i}],
\end{equation}
where~$s^{(m)}(t)$ is an element of time series~$\bs^{(m)}$. Select $t_i$, $i = 1, \dots, q$ from $\Gs = \{1, \dots, T_{\max}\}$ so that vectors $\bs_i = [\y_i, \x_i]$ cover time series $\bs$
\begin{equation}\label{eq:strategy2} \{s(1), \dots, s(T_{\max})\} = \bigcup_{i=1}^{q-1} \{s(t_i-\Delta t_\text{r}),\dots,s(t_i)\}\end{equation}
with overlap in feature parts $\bx_i$ and no intersection in target parts  $\y_i$:
\[ |t_{i+1} - t_i| > \dtr.
\]
Following~\eqref{eq:segment} and~\eqref{eq:strategy2}, extract segments $i = 1, \dots, q$ from all time series $\bs^{(m)} \in \fD$ and form the matrix
 \[
\bX^*=\left[
\begin{array}{lll|lll}
\by^{(1)} &  \dots & {\by}^{(M)} & \bx_0^{(1)} & \dots & \bx_0^{(M)}  \\
\hline
\by_1^{(1)} &  \dots & \by_1^{(M)} & \bx_1^{(1)}  & \dots & \bx_1^{(M)}  \\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots \\
\by_q^{(1)}  & \dots & \by_q^{(M)} & \bx_q^{(1)} & \dots & \bx_q^{(M)}  \\
\end{array}
\right]
=\left[
\begin{array}{c|c}
 \underset{1{\times}r}{{\by}_0} & \underset{1{\times}n}{\x_0} \\
 \hline
 \underset{q{\times}r}\bY & \underset{q{\times}n}\bX \\
\end{array}
\right].
\]
Denote a row from the pair~$\bY,\bX$ as $\by,\bx$ and call these vectors the target and the features. 

\paragraph{Feature selection and multimodelling.}
As one must forecast all elements from the target~$\by$, only a few elements from the features~$\bx$ are supposed to be informative in terms of the forecast quality. Denote the index set~$\cJ=\{1,\dots,n\}$ and select the subset of the indexes~$\cA\in\cJ$.
Introduce the forecasting model
\[
\hat{\by}_i = \sum_{k=1}^K \pi_{ik} \mathbf{f}_k  (\bw_{\mathcal{A}_k}, \bx_{i\mathcal{A}_k})
\]
as some linear combination of~$K$ models and call it the \emph{multimodel}. Each model~$\fx_k$ has its parameters~$\bw_k$ and selected features~$\bx_{\cA_k}$. The coefficient~$\pi_{ik}$ set a vector~$ \bx_i$ in correspondence to the  model~$\fx_k$, so that
\[
\sum_{k=1}^K \pi_{ik}  = 1 \quad\text{for}\quad i\in\cI = \{1,\dots,m\}
\]
with two options are to be considered:~$\pi\in\{0,1\}$ and~$\pi\in[0,1]$.
Let the forecasting error be
\[
S=\sum_{i\in\cB_0} \|\hat{\by}_i - \by_i\|_1,
\]
where the set of object indexes~$\cI$ is splatted to the test set~$\cB_0$ and the train sets,
\[
\cI = \cB_0\mathop{\sqcup}\limits_{k=1}^K \cB_k.
\]

State the forecasting problem as a problem to minimize the error function~$S$ given models~$\fx_1,\dots,\fx_K$ by optimizing matrix $\Pi = [\pi_{ik}]$, finite sets~$\cA_1,\dots,\cA_K$ and model parameters~\hl{$\bw_1,\dots,\bw_K$} on the sample set with indexes~$\cI\setminus\cB_0$.

\subsection{Special case of the problem}
A special case of the problem is an early warning forecasting. There is a special time series~$\bar{\bs}$ with its elements~$\bar{s}\in\{0,1\}$. Here zero is interpreted as a \emph{normal state} of the system and one meas the system goes from normal to the \emph{abnormal} state without return over time~$t$.
The problem is to maximize the lapse of the time segment
\[ \| \dtr \| \to\max,\]
where the vector
\[
[{\y}_0, \bx_0] = \textcolor{red}{[0,\dots,0,0,\dots,0,1]},
\]
which means the system was in the normal state before it changes. Since the quality~$Q$ of forecasting time series~$\bar{\bs}$ depends on~$\| \dtr \|$ (the letter time lapse before the warning the higher the forecasting quality~[ref]) the minimum level of quality must be set. Let the minimum forecasting quality be
\[
Q \left\{
\bigl(\hat{\by}_i,\bar{\by}_i\bigr)
\;|\;
i\in\cB_0
\right\} = \text{AUC} = Q_\text{req}.
\]

\begin{figure}[!ht]
\centering\includegraphics[width=0.8\textwidth]{retrospective_validation.png}
\caption{\hl{Retrospective forecast includes most recent samples in data set.}\label{fg:retrospective}}
\end{figure}

\paragraph{Forecast analysis.} We consider the forecast testing procedure, given by the algorithm~\ref{alg:train_test_rmse}. Since our ultimate goal is to construct a forecasting model $\fx$ be able to obtain forecasts $\hat{y}_i$ at any given time $t \leq t_i + \dtr$ we have to imitate this setting, using the so-called retrospective forecast~\ref{fg:retrospective} (\hl{rolling forecasts, walking ahead predictions}). Here we conceal most recent historical samples $\bx_0^{*}$ and make predictions as if they were unknown. Then the quality of the model $\fx$ is evaluated according to its performance on these concealed samples. 


\begin{algorithm}[!h]
\textbf{\emph{ComputeForecastingErrors()}}

 \KwData{$\bX^{*} \in \mathbb{R}^{M\times(\dtr+\dtp)}$. Parameters: sample size $m$, train to test ratio $\alpha$.}
 \KwResult{Forecasting quality: root-mean-squared error.}
 \While{$n \leq M - m$:}{
 define, $\bX^{*}_n = [\bx^{*}_{n}, \dots, \bx^{*}_{m + n - 1}]\T$ \;
  $\bX_{\text{train}}, \; \bX_{\text{test}}, \;\bX_{\text{val}} = TrainTestSplit(\bX^{*}_n, \alpha)$\;
  train forecasting model $\fx(\x, \hat{\w}_n)$, using $\bX_{\text{train}}$ and $\bX_{\text{test}}$\;
  obtain vector of residuals $\veps = [\varepsilon_{T}, \dots, \varepsilon_{T- \dtr + 1}]$ with respect to $\bX_{\text{val}}$ \;
  compute forecasting quality:
  \[ \text{RMSE}(n)  = \sqrt{\frac{1}{\dtr} \sum_{t=0}^{\dtr} \varepsilon_{T-t}^2};\]
  $n = n + 1$ \;
  }
  Average RMSE by data splits.
  \bigskip

\textbf{\emph{TrainTestSplit()}}

\KwData{Object-feature matrix $\bX^{*} \in \mathbb{R}^{m\times(\dtr+\dtp)}$. Train to test ratio $\alpha$.}
 \KwResult{Train, test, validation matrices $\bX^{*}_{\text{train}}$, $\bX^{*}_{\text{test}}$, $\bX^{*}_{\text{val}}$.}
 Set train set and test set sizes:

 $ \quad m_{\text{train}} = \lfloor\alpha(m-1)\rfloor$ \;
 $ \quad m_{\text{test}} = m - 1 - m_{\text{train}} $ \; %(1-\alpha)(m-1)
 Decompose matrix $\bX^{*}$ into train, test, validation matrices $\bX^{*}_{\text{train}}$, $\bX^{*}_{\text{test}}$, $\bX^{*}_{\text{val}}$:
 \[\bX^{*}_{\text{train}} = \left[\begin{array}{c} \x^{*}_{\text{val}} \in \mathbb{R}^{1\times (\dtr + \dtp)}\\
 \hline
 \bX^{*}_{m_{\text{test}}} \in \mathbb{R}^{m_{\text{test}} \times (\dtr + \dtp)} \\
 \hdashline
 \bX^{*}_{m_{\text{train}}} \in \mathbb{R}^{m_{\text{train}} \times (\dtr + \dtp)}
 \end{array}\right] = \left[\begin{array}{c|c} \y_{\text{val}} & \x_{\text{val}} \\
 \hline
 \bY_{m_{\text{test}}} & \bX_{m_{\text{test}}}  \\
 \hdashline
 \bY_{m_{\text{train}}}  & \bX_{m_{\text{train}}}
 \end{array}\right]
 \]
 %$\bX^{*}_{\text{train}} = \left[\begin{array}{c|c} \y_1 & \x_1 \\ \dots &\dots, \\
% \y_{m_{\text{train}}} & \x_{m_{\text{train}}} \end{array}\right], ~~
% \bX^{*}_{\text{test}} = \left[\begin{array}{c|c} \y_1 & \x_1 \\ \dots & \dots, \\
% \y_{m_{\text{test}}}& \x_{m_{\text{test}}} \end{array}\right] $ \;
 \caption{Train-test split.}\label{alg:train_test_rmse}
\end{algorithm}

\section{Feature generation}
List of procedures for constructing the feature and the object sets will be placed here. Discussion point: vector~$\y$ {\bf remains always unchanged}. \\
The feature set $\mathcal{J} = \mathop{\cup}\limits_k \mathcal{A}_k$ includes
\begin{enumerate}[1)]
\item the local history of all time series themselves,
\item transformations (non-parametric and parametric) of local history,
\item parameters of the local models,
\item distances to the centroids of local clusters.
\end{enumerate}
The object set  $\mathcal {I} = \mathop{\sqcup}\limits_k \mathcal{B}_k$ includes
\begin{enumerate}[1)]
\item the local history,
\item parametric local models  and their residuals (including ones from previous iterations),
\item DTW-shifted local history as a local forecasting procedure,
\item aggregated subsets of time series.
\end{enumerate}

Denote the generated feature vector as~$\vf$. This vector consists of concatenated row-vectors~$\vf=[\vf^{(1)},\dots,\vf^{(M)}]$, which corresponds to time series local histories ~$\bs=[\bs^{(1)},\dots,\bs^{(M)}]$, modified with set of transformations~$\mathfrak{G}$. The elements~$g:\bs\to\vf$ of this set are listed below.

\subsection{Transformations of local history}
The tables~\ref{tb:musttries},~\ref{tb:elementaries},~\ref{tb:monotones},~\ref{tb:multivariates},~\ref{tb:datastats} list the~time series transformation functions. There are non-parametric and parametric procedures to generate features. For the parametric functions~$g=g(\bb,s)$ the default values of the parameters~$\bb$ are assigned empirically.

The parametric procedure request two optimization problem statements of the model parameters~$\bw$ and the primitive function parameters~$\bb$. The first one fixes the vector~$\hat\bb$, collected over all the primitive functions~$\{g\}$, which generate features~$\vf$:
\[
\hat\bw = \arg\min S\bigl(\bw|\fx(\bw,\bx),\by\bigr),\quad
\text{where}
\quad [\by,\bx] = \vf(\hat\bb,\bs).
\]
The second one optimizes the transformation parameters~$\hat\bb$ given obtained model parameters~$\bw$
\[
\hat\bb = \arg\min S\bigl(\bb|\fx(\hat\bw,\bx),\by\bigr).
\]
This procedure repeats two problems until vectors~$\hat\bw,\hat\bb$ converge. \hl{The initial values of vector~$\bb$ (are shown in table}~\ref{tb:primitives}). Due to the various origins of the time series and their transformations the residual vector should be normalized:
\[\veps = \frac{\hat\by-\by}{|\by| \cdot \|\by\|_2^1}.
\]
\hl{It (transformation? normalization)} does not change the number elements in the vectors,~$|\vf|=|\bs|$.

\subsection{Convolutions, statistics and parameters of local history}
The listed feature generation functions convolves time series, so they reduce the dimensionality $|\vf=\bg(\bs)|<|\bs|$.

\subsection{Parameters of local history forecast}
For the time series~$\bs$ construct the Hankel matrix with a~period~$k$ and shift~$p$, so~that for~$\bs = [s_1,\dots,s_T]$ the matrix
\[
\bH^* =
\left[ \begin{array}{c|cc}
s_T  & \dots & s_{T-k+1} \\
\vdots & \ddots & \vdots \\
s_{k+p} & \dots & s_{1+p}\\
s_k & \dots & s_1 \\
\end{array}
\right],
\text{~where~} 1\geqslant p \geqslant k .
\]
Reconstruct the regression to the first column of the matrix~$\bH^*=[\bh, \bH]$ and denote its least square parameters as the feature vector
\[
\boldsymbol{\phi}^{(m)} = \arg\min \| \bh-\bH \boldsymbol{\phi}\|_2^2.
\]
For the time series~$\bs^{(m)}$, $m=1,\dots, M$ use the parameters $\boldsymbol{\phi}^{(m)}$ as the features.

\subsection{Distances to centroids of local clusters}
This procedure applies the kernel trick to the time series. For given local history time series~$\bx_i^{(m)}$, $m=1,\dots, M$ compute $k$-means centroids~$\bc_p^{(m)}$, $p = 1, \dots, P$.  With the selected $k$-means distance function~$\rho$ construct the feature vector
\[
\vf_i^{(m)} = [\rho(\bc_1^{(m)},\bs_i^{(m)}),\dots,\rho(\bc^{(m)}_P,\bs_i^{(m)})] \in \R_+^P.
\]
This $k$-means of another clustering procedure may use internal parameters, so that there are no parameters to be included to the feature vector or to the forecasting model.

\begin{table}[!ht]
\caption{Must-try functions.}
\label{tb:musttries}
\begin{tabular}{|c|p{50pt}|p{50pt}|p{50pt}|}
\hline
Formula		&	Output dimension	&	$\#$ of arguments	&	$\#$ of parameters	\\ \hline
$	\sqrt{x}	$	&	1	&	1	&	0	\\ \hline
$	x\sqrt{x}	$	&	1	&	1	&	0	\\ \hline
$	\arctan{x}	$	&	1	&	1	&	0	\\ \hline
$	\ln{x}	$	&	1	&	1	&	0	\\ \hline
$	x\ln{x}	$	&	1	&	1	&	0	\\ \hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\caption{List of elementary functions.}
\label{tb:elementaries}
\begin{tabular}{|p{90pt}|c|p{50pt}|p{50pt}|p{50pt}|}
\hline
Function name	&		Formula		&	Output dimension	&	$\#$ of arguments	&	$\#$ of parameters	\\ \hline
Add constant	&	$	x + w	$	&	1	&	1	&	1	\\ \hline
Quadratic	&	$	w_2 x^2 + w_1 x + w_0	$	&	1	&	1	&	3	\\ \hline
Cubic	&	$	w_3x^3 + w_2 x^2 + w_1 x + w_0	$	&	1	&	1	&	4	\\ \hline
%Nonparametric sin	&	$	\sin(x)	$	&	1	&	1	&	0	\\ \hline
%Sin	&	$	\sin(w_0 + w_1x)	$	&	1	&	1	&	2	\\ \hline
%Square root	&	$	\sqrt{x}, \;x\sqrt{x}	$	&	1	&	1	&	0	\\ \hline
%Arctangent	&	$	\arctan{x}	$	&	1	&	1	&	0	\\ \hline
%Logarithmic	&	$	\ln{x}, \;x\ln{x}	$	&	1	&	1	&	0	\\ \hline
Logarithmic sigmoid	&	$	1/(w_0 + \exp(-w_1x))	$	&	1	&	1	&	2	\\ \hline
Exponent	&	$	\exp{x}	$	&	1	&	1	&	0	\\ \hline
Normal	&	$	\frac{1}{w_1\sqrt{2\pi}}\exp\left(\frac{(x-w_2)^2}{2w_1^2}\right)	$	&	1	&	1	&	2	\\ \hline
Multiply by constant	&	$	x\cdot w	$	&	1	&	1	&	1	\\ \hline
Monomial	&	$	w_1 x^{w_2}	$	&	1	&	1	&	2	\\ \hline
Weibull-2	&	$	w_1w_2x^{w_2-1}\exp{-w_1x^{w_2}}	$	&	1	&	1	&	2	\\ \hline
Weibull-3	&	$	w_1w_2x^{w_2-1}\exp{-w_1(x-w_3)^{w_2}}	$	&	1	&	1	&	3	\\ \hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\caption{Monotone functions.}
\label{tb:monotones}
\begin{tabular}{|p{90pt}|c|p{50pt}|p{50pt}|p{50pt}|p{60pt}|}
\hline
\multicolumn{6}{|c|}{By growth rate} \\
\hline
Function name	&		Formula		&	Output dimension	&	$\#$ of arguments	&	$\#$ of parameters	&	Constraints	\\ \hline
Linear	&	$	w_1 x + w_0	$	&	1	&	1	&	2	\\ \hline
Exponential rate	&	$	\exp(w_1x + w_0)	$	&	1	&	1	&	2	&	$w_1 > 0$	\\ \hline
Polynomial rate	&	$	\exp(w_1\ln x + w_0)	$	&	1	&	1	&	2	&	$w_1 > 1$	\\ \hline
Sublinear polynomial rate	&	$\exp(w_1\ln x + w_0)$	&	1	&	1	&	2	&	$0 < w_1 < 1$	\\ \hline
Logarithmic rate	&	$	w_1\ln x + w_0	$	&	1	&	1	&	2	&	$w_1 > 0$	\\ \hline
Slow convergence	&	$	w_0 + w_1/x	$	&	1	&	1	&	2	&	$w_1 \neq 0$	\\ \hline
Fast convergence	&	$	w_0 + w_1\cdot\exp(-x)	$	&	1	&	1	&	2	&	$w_1 \neq 0 $	\\
\hline
\multicolumn{6}{|c|}{Other} \\
\hline
Soft ReLu & $\ln(1+e^{x}) $	&	1	&	1	&	0	&		\\ \hline
Sigmoid	&	$	1/(w_0 + \exp(-w_1x))	$	&	1	&	1	&	2	&	$w_1 > 0$	\\ \hline
Nonparametric log-sigmoid	&	$	1/(1 + \exp(-x))	$	&	1	&	1	&	0	& \\ \hline
Hiberbolic tangent	&	$	\tanh(x)	$	&	1	&	1	&	0 & 	\\ \hline
% &   $-1,\text{~for~} x < -1$ &		&		&		&		\\
%hardtan &  $0, \text{~if~} -1\leq x\leq 1, $ &	1	&	1	&	0	&		\\
% &  $1,\text{~for~} x > 1$ &		&		&		&		\\ \hline
softsign & $\frac{|x|}{1+|x|}$	&	1	&	1	&	0	&		\\ \hline
\end{tabular}
\end{table}


\begin{table}[!ht]
\caption{Multivariate.}
\label{tb:multivariates}
\begin{tabular}{|p{90pt}|c|p{50pt}|p{50pt}|p{50pt}|}
\hline
\multicolumn{5}{|c|}{Bivariate} \\
\hline
Plus	&	$	x_1 + x_2	$	&	1	&	2	&	0	\\ \hline
Minus	&	$	x_1 - x_2	$	&	1	&	2	&	0	\\ \hline
Product	&	$	x_1 \cdot x_2	$	&	1	&	2	&	0	\\ \hline
Division	&	$	\frac{x_1}{x_2}	$	&	1	&	2	&	0	\\ \hline
	&	$	x_1\sqrt{x_2}	$	&	1	&	2	&	0	\\ \hline
	&	$	x_1\ln{x_2}	$	&	1	&	2	&	0	\\ \hline
\multicolumn{5}{|c|}{Multivariate} \\
\hline
Sum of products	&	$\sum_{i,\;j}x_ix_j	$	&	1	&	$n\geq 2$	&	0	\\ \hline
Sum of products	&	$\sum_{i,\;j,\;k}x_ix_jx_k$	&	1	&	$n\geq 3$	&	0	\\ \hline
Sum of Gaussians	&	$	\sum_{j=1}^n a_j\exp(-\frac{(x_j-b_j)^2}{c_j})	$	&	1	&	$n$	&	$3n$	\\ \hline
Polynomial	&	$	\sum_{j=0}^n a_jx^j	$	&	1	&	1	&	$n$	\\ \hline
Rational polynomial	&	$	\frac{\sum_{j=0}^n a_jx^j}{x^m + \sum_{j=0}^{m-1}b_jx^j}	$	&	1	&	1	&	$n + m + 1$	\\ \hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\caption{Data statistics.}
\label{tb:datastats}
\begin{tabular}{|p{90pt}|c|p{50pt}|p{50pt}|p{50pt}|p{60pt}|}
\hline
sum	&	$	\sum_i x_i	$	&	1	&	$m$	&	0	\\ \hline
mean	&	$	(\sum_i x_i)/m	$	&	1	&	$m$	&	0	\\ \hline
min	&	$	\min_i x_i	$	&	1	&	$m$	&	0	\\ \hline
max	&	$	\max_i x_i	$	&	1	&	$m$	&	0	\\ \hline
std	&	$	\frac{1}{m-1}\sqrt{\sum_i(x_i - \text{mean}(x))^2}	$	&	1	&	$m$	&	0	\\ \hline
hist &	$	\sum_i [X_{j-1} < x_i \leq X_j ]	$	&	$n$	&	$m$	&	$n-1$	\\ \hline
conv &	$	\sum_j x_{i-j}w_j 	$	&	1	&	$m- n + 1$	&	$n \leq m$	\\ \hline
\multicolumn{2}{|c|}{FFT coefficients}	&	$n$	&	$m$	&	1	\\ \hline

\end{tabular}
\end{table}

\section{Feature selection}
TODO

\section{Mixture models}
Let $D = (X, \by)$ denote the data, where $X = [\bx_1\T, \dots, \bx_i, \dots, \bx_m\T]\T$,
 denotes the inputs $\bx_i\in\R^{n}$, $\by$ denotes the targets $y_i \in Y$. The task is to estimate $y_i$, given $\bx_i$. Assuming linear model $f$ with gaussian noise
 \begin{equation*}
 y = f(\x, \w) + \varepsilon, \quad f(\x, \w) = \w\T\x, \; \varepsilon \sim \mathcal{N}(0,\beta) \Rightarrow y \sim \mathcal{N}(\w\T\x, \beta),
 \end{equation*}
 obtain the maximum likelihood estimate
 \[\hat{y} = \hat{\w}\T\x, \quad \hat{\w} = \argmax_{\w} \frac{1}{2\beta}\sum_{i=1}^{m}(y_i - \w\T \x_i)^2 \]
 for the output.

% Maximum likelihood estimation gives
%\[ \hat{y} = \argmax_{y in Y} P(y|\bx, \w).\]
\subsection{EM-algorithm for mixture models}
Assume the target variable $\by$ is generated by one of~$K$ linear models $f_{k}(\x, \w_k)$. Let the distribution of the target variable~$\by$ be a mixture of normal distributions
\begin{equation}\label{eq:mixture_models}
p(\by|\x, \bth) = \sum_{k=1}^{K}\pi_k\;\mathcal{N}(\by|\w_k\T\bx,\beta) = \sum_{k=1}^K
\frac{1}{(2\pi\beta_k)^{n/2}} \exp \left(
( -\frac{1}{2\beta_k}(\by - \w_k\T X)^{\top}(\by - \w_k\T X) \right).
\end{equation}
Here~$\bth$ denotes the concatenated vector of parameters:
\[
\bth = [\w_1,\dots,\w_k,\boldsymbol{\pi},\beta]\T,
\]
where $\boldsymbol{\pi} = [\pi_1,\dots,\pi_k]$ are weights of the models,
and $\bB = \beta \bI_m$ is the covariance matrix for~$\by$.

\paragraph{Parameter estimation.}
The goal is to find parameters vector $\hat{\bth}$ which optimizes loglikelihood function for given data set~$D$
\begin{equation}\label{eq:MLE}
\hat{\bth} = \argmax_{\bth}\ln p(\by|\bth), \quad \ln p(\by|\bth) = \sum_{i=1}^{m}\ln\left( \sum_{k=1}^K  \pi_k\mathcal{N}(\by|\w\T_{k}\x_i,\beta) \right).
\end{equation}
%To estimate parameters $\bth$ introduce the matrix
%\[
%Z = \left[\bz_1,\dots,\bz_m|\bz\in\{0,1\}^K\right]
%\]
%of hidden variables. A hidden variable $\bz_i$ indicates which model generates $\mathbf{x}_i$: $\mathbf{x}_i$ is generated by $k$-th model iff $\bz_{ik}=\delta_{ik}$.
%The log-likelihood function for joint distribution of~$\y,Z$ is
%\begin{equation}\label{eq:joint_y_z}
%\ln p(\by,Z|\bth) = \sum_{i=1}^m\sum_{k=1}^K z_{ik} \ln \left( \pi_k \mathcal{N}(y_i|\w\T_{k}\x_i,\beta) \right).
%\end{equation}


To obtain maximum likelihood estimates~\eqref{eq:MLE} for parameter $\bth$ of the model~\eqref{eq:mixture_models}, let us introduce hidden indicator variables
\[Z = \left[\bz_1,\dots,\bz_m\right], \quad z_{ik}\in \{0,1\},\] such that
\[z_{ik} = 1 \Leftrightarrow y_i\sim\mathcal{N}(\w_k\T\x_i, \beta).\]
Then the loglikelihood function $p(\by, Z|X, \bth)$ takes the form
\[p(\by|X, Z, \bth) = \sum_{i=1}^{m}\sum_{k=1}^{K} z_{ik} \left( \ln\pi_k + \ln\mathcal{N}(y_i|\w\T_{k}\x_i,\beta) \right) = \]
\[=\sum_{i=1}^{m}\sum_{k=1}^{K} z_{ik} \left( \ln\pi_k - \frac{1}{2\beta}(y_i-\w\T_{k}\x_i)^2 + \frac{n\ln{\beta}}{2} + \text{const}\right).\]
Since $p(\by, Z|X, \bth)$ depends on random variables $z_{ik}$, instead of $p(\by|X, \bth)$ maximize the expected loglikelihood of the observed data $D$:
\[\mathsf{E}_{Z} [p(\y, Z|X, \bth)] = \sum_{i=1}^{m}\sum_{k=1}^{K} \gamma_{ik} \left( \ln\pi_k - \frac{1}{2\beta}(y_i-\w\T_{k}\x_i)^2 + \frac{n\ln{\beta}}{2}\right), \quad \gamma_{ik} = \mathsf{E}[z_{ik}|\y, X].\]


Finally, apply Expectation-Maximization algorithm to maximize $\mathsf{E}_{Z} [p(\y, Z|X, \bth)]$ updating parameters estimates $\bth^{(r)}$ in two iterative steps.


{\bf E-step}: obtain $\mathsf{E}(Z)$. Let~$\Gamma=[\gamma_{ik}]$ be a matrix of posterior probabilities that $i$-th sample is generated by~$k$-th model. Using Bayesian rule, obtain
\begin{equation}\label{eq:hidden_vars_Estep}
\gamma_{ik}^{(r+1)} = \mathsf{E}(z_{ik}) = p(k|\x_i, \bth^{(r)}) =
    \frac{
        \pi_k\mathcal{N}(y_i|\x_i\T\w^{(r)}_{k}, \beta^{(r)})
        }{
        \sum_{k'=1}^{K}\pi_{k'}\mathcal{N}(y_i|\x_i\T\w^{(r)}_{k}, \beta^{(r)}).
        }
\end{equation}

Define expectations of joint loglikelihood $\ln p(\y, Z|X, \bth)$ with respect to the posteriors distribution~$p(Z|\y,\bth)$
\begin{equation}\label{eq:posterior}
Q^{(r)}(\bth) = \mathsf{E}_Z(\ln p(\y, Z|\bth)) = \sum_{i=1}^m\sum_{k=1}^K\gamma_{ik}^{(r+1)}
    \left(
        \ln\pi^{(r)}_k+\ln\mathcal{N}(y_i|\x_i\T\w^{(r)}_{k},\beta^{(r)})
    \right).
\end{equation}

{\bf M-step:} update parameters $\bth$, maximizing $Q^{(r)}(\bth)$. Maximize function~$Q^{(r)}(\bth)$ with respect to~$\bth$ with~$\Gamma^{(r+1)}$ fixed. First, optimize $\pi_k$, which is constrained as $\sum_{k=1}^K\pi_k=1$. Using Lagrange multipliers, obtain the following estimation
\[
\pi^{(r+1)}_k = \frac{1}{n}\sum_{i=1}^m { \gamma_{ik}^{(r+1)}}.
\]

Next, maximize~$Q^{(r)}$ with respect to~$\w_k$ for~$k$-th model.
With $\pi_k$ fixed maximizing~\eqref{eq:posterior} is equivalent to \
\[
\w^{(r+1)}_k = \argmax_{\w_k}\sum_{i=1}^m -\gamma^{(r+1)}_{ik}
    \left(
        y_i-\w_k\T\x_i
    \right)^2,
\]
\[
\beta^{(r)}_k = \argmax_{\beta}\sum_{i=1}^m\gamma^{(r+1)}_{ik}
    \left(n\ln\beta
        -\frac{1}{\beta}(y_i-\x_i\T\w^{(r+1)}_k)^2
    \right).
\]
%The constant term measures the of $k-$th models, $k\neq i$ into~$Q$.
\subsection{Mixture of experts}
Suppose that each model $f(\x, \w_k)$ generates a sample $(\x, y)$ with some probability $p(k|\x, \w)$. Then
the following factorization holds
\begin{equation*}\label{eq:mixture_of_experts} p(y|\x, \bth) = \sum_{k=1}^{K} p(y, k| \x, \bth) = \sum_{k=1}^{K} p(k|\x, \bth)p(y|k, \x, \bth)\end{equation*}
 for $p(y|\x, \bth)$. Here $p(k|\x, \bth)$ correspond to weight parameters $\pi_k$ in~\eqref{eq:mixture_models} dependent on the inputs $\x$.
 Assuming normal linear models $f(\x, \w_k)$ or, equivalently, normal  distributions
 $p(y|\bx, \w_k) = \mathcal{N}(y|\w_k\T, \beta),$
 obtain
\begin{equation}\label{eq:mixture_of_experts}p(\by|\x, \bth) = \sum_{k=1}^{K}\pi_k(\x, \bv_k)\mathcal{N}(\by|\w_k\T\bx,\beta), \end{equation}
where
\[\pi_k(\x, \bv_k) = \frac{\exp(\bv_k\T\x)}{\sum_{k'=1}^K \exp(\bv_{k'}\T\x)}.\]
   The difference between mixture of experts model~\eqref{eq:mixture_of_experts} and mixture model~\eqref{eq:mixture_models} in that model weights $\pi_k$ depend on inputs $\x$ in mixture of experts. Similarly, EM-procedure for mixture of experts differs from EM-procedure for mixture models in the way $\gamma_{ik}$ are optimized in M-step.

\begin{algorithm}[ht]
 \KwData{$(\x_i, y_i)$, $i = 1, \dots, m$. Parameters: number of experts $K$.}
 \KwResult{Parameters $\bth$ of the model~\eqref{eq:mixture_of_experts}.}
 Initialize $[\w, \beta, \bv] \equiv \bth = \bth^{(0)}$, $r = 0$\;
 \While{$\bth$ keeps changing}{
  \textbf{E step}: compute hidden variables $\gamma^{(r+1)}_{ik}$, the expectation of the indicator
variables, using~\eqref{eq:hidden_vars_Estep}\;

  \textbf{M step}: find new parameter estimates
  $$ \bv^{(r+1)}_k = \argmax_{\bv} Q^{(r),\bv}_k(\bv), \quad Q^{(r),\bv}_k(\bv) = \sum_{i=1}^m \gamma_{ik}^{(r+1)}\ln\pi_k(\x_i, \bv)$$
  $$ \w^{(r+1)}_k = \argmax_{\w_k} Q^{(r),\w}_k(\w_k), \quad Q^{(r),\w}_k(\w_k) = \sum_{i=1}^m\gamma^{(r+1)}_{ik}
    \left(
        y_i-\w_k\T\x_i
    \right)^2, $$
  $$ \beta^{(r+1)}_k = \argmax_{\beta} Q^{(r),\beta}_k(\beta), \quad Q^{(r),\beta}_k(\beta) =\left(n\ln\beta
        -\frac{1}{\beta}(y_i-\x_i\T\w^{(r+1)}_k)^2
    \right)
    \;$$
 }
 \caption{EM-algorithm for mixture of experts.}
\end{algorithm}

\subsection{Distance between two models of N time series}

Introduce a distance function~ $\rho(f_k,f_l)$ between two models.
Use the Jensen-Shannon divergence;~$\rho_{kl}\in[0,1]$ is a~metric:
\[
\rho(p_k\|p_l) = 2^{-1}\KL{p_k}{p'} + 2^{-1} \KL{p'}{p_l},
\]
where $p' = 2^{-1}(p_k + p_l)$ and~$p_k\eqdef(p(\w|D,A,B,f_k)$.
The non-symmetric Kullback-Leibler divergence is
\[
\KL{p}{p'}=\int\limits_{\bw\in\mathbb{W}} p'(\bw) \ln \frac{p(\bw)}{p'(\bw)}d\bw.
\]

\section{Computational experiment}
The goal of the experiment is to compare the following four approaches to the multiscale forecasting:
1) Bayesian mixture model approach,
2) random multumodel,
3) vector random decision forest and
4) vector adaboost.
The last two algorithms are modifications of \hl{[link]}. The modifications are needed to produce the vector of multiscale time series as their outputs. The experiment is performed on 1) non-modified autoregression data and on 2) data with additionally generated features as it is described in the corresponding section.

\begin{figure}[!t]
\centering\includegraphics[width=0.9\textwidth]{02_A0.png}
\label{fg:IDEF}
\caption{Multiscale forecasting pipeline.}
\end{figure}

\section{Appendix: Discrete genetic algorithm for feature selection (will be converted to bootstrap random linear multimodel algorithm)}
\begin{enumerate}
\item There are set of binary vectors $\{\mathbf{a}_1,\ldots,\mathbf{a}_P\}$, $\mathbf{a}\in\{0,1\}^n$;
\item get two vectors $\mathbf{a}_p, \mathbf{a}_q$, $p,q\in\{1,\ldots,P\}$;
\item chose random number $\nu\in \{1,\ldots,n-1\}$;
\item split both vectors and change their parts:
$$[a_{p,1},\ldots,a_{p,\nu},a_{q,\nu+1},\ldots,a_{q,n}]\to\mathbf{a'}_p,$$
$$[a_{q,1},\ldots,a_{q,\nu},a_{p,\nu+1},\ldots,a_{p,n}]\to\mathbf{a'}_q;$$
\item choose random numbers $\eta_1,\ldots,\eta_Q\in\{1,\ldots,n\}$;
\item invert positions $\eta_1,\ldots,\eta_Q$ of the vectors $\mathbf{a'}_p,\mathbf{a'}_q$;
\item repeat items 2-6 $P/2$ times;
\item evaluate the obtained models.
\end{enumerate}
Repeat $R$~times; here $P,Q,R$ are the parameters of the algorithm and~$n$ is the number of the corresponding model features.

\section{Appendix: Mixture modelling under random bootstrapped models}
Denote the indexes of objects as $\{1,\ldots,i,\ldots,m\}=\I$, the split $\cI=\cB_1\sqcup\dots\sqcup\Bi_K$ and the indexes of  features as $\{1,\ldots,j,\ldots,n\}=\cJ$, the active set $\cA_k\subseteq\cJ$.

Let the regression model
\[
\fx:(\bw,\bx) \mapsto \by;
\]
with the selected model of optimal structure
\[
\quad \mathsf{E}(\by_i|\bx) = \bW_\cA\bx_i.
\]
The multimodel~$\mathfrak{f}$ is a set of the models~$\mathfrak{f}=\{\fx_k\quad | k=1,\ldots,K\}$, such that for each~$k$
\[
\mathsf{E}(y_{i\in\cB_k}|\bx) = \bW_{\cA_k}\bx_{i\in\cB_k}
\qquad\text{with}\quad \cI=\sqcup_{k=1}^K\cB_k\ni{i}.
\]
State the multimodel selection problem as follows. An optimal single model is
\[
\hat{\fx}(\bw,\bx) = \arg\max\limits_{\cA\subseteq\cJ}\mathcal{E}\left(\fx(\bw_\cA,\bx)\right),
\]
where~$\mathcal{E}$ denotes the model evidence in coherent Bayesian inference.
An optimal multilevel model i s
\[
\hat{\mathfrak{f}}(\bw_1,\ldots,\bw_K,\bx) = \arg\max\limits_{\sqcup_{k=1}^K\cB_k=\cI}\prod_{k=1}^K\mathcal{E}\left(\fx(\bw_k,\x_{\cB_k})\right).
\]

\sout{The model difference must be statistically significant}
\[
\mathcal{F}\supset\hat{\mathfrak{f}} = \arg\max\limits_{\cB_1,\cB_2\subset\cB}
\rho(f_1,f_2)
\]
\sout{given set of indices~$\hat{\Aj}$, such that}
\[
\hat{\cA} = \arg\max\limits_{\cA\subseteq\cJ}\mathcal{E}\left(\fx_1(\bw_\cA,\x^{\cB_1})\right)\mathcal{E}\left(\fx_2(\bw'_\cA,\bx^{\cB_2})\right).
\]



\end{document}

Constructing the object dictionary

According to the hypothesis ``the deeper history the lesser its impact to the future start forming the sample set~$X$ form the present time to the past expecting consequential augmentation of the forecasting quality.
\begin{def}
Call the necessary sample size the size when the augmentation of the forecasting quality probabilistically converges on the sample set increasing. Note that the sample set must be simple: there exist a single adequate model, which approximates the whole set.
\end{def}

\begin{def} Call the model stability the
\end{def}

\begin{def} Call the multimodel stability
\end{def}

Basic model
(Short description of the random forest)



\end{document} 