\documentclass[12pt]{article}
\usepackage[english,russian]{babel}
%\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
%\usepackage[utf8]{inputenx}
%\usepackage{jmlda}
\usepackage[]{cite}
\usepackage[]{algorithm2e}
\usepackage[noend]{algorithmic}
\usepackage{colortbl}
\usepackage{amsmath,mathrsfs,amsfonts,amssymb}
\usepackage{graphics,graphicx,epsfig}
\usepackage{epstopdf}
\usepackage{subfig}
\usepackage{enumerate}
\usepackage{pdflscape}
\usepackage{NotationStyle}
\textheight=240mm % высота текста
\textwidth=170mm % ширина текста
\oddsidemargin=0mm % отступ от левого края
\topmargin=0mm % отступ от верхнего края
\headsep=0mm
\headheight=0mm

\renewcommand{\bs}{\boldsymbol{\theta}}
%\renewcommand{\baselinestretch}{1.3}
%
%
%\definecolor{grr}{rgb}{0.5,0.5,0.5}
%\newcommand\TODO[1]{\textcolor{red}{#1}}
%\newcommand{\x}{\mathbf{x}}
%\newcommand{\by}{\mathbf{y}}
%\newcommand{\R}{\mathbb{R}}
%\newcommand{\fx}{\mathbf{f}}
%\newcommand{\w}{\mathbf{w}}
%\newcommand{\T}{{\text{\tiny\sffamily\upshape\mdseries T}}}
\newcommand{\argmax}{\mathop{\rm arg\,max}\limits}


\begin{document}
Let $D = (X, \by)$ denote the data, where $X = [\bx_1\T, \dots, \bx_i, \dots, \bx_m\T]\T$,
 denotes the inputs $\bx_i\in\R^{n}$, $\by$ denotes the targets $y_i \in Y$. The task is to estimate $y_i$, given $\bx_i$. Assuming linear model $f$ with gaussian noise
 \begin{equation*}
 y = f(\x, \w) + \varepsilon, \quad f(\x, \w) = \w\T\x, \; \varepsilon \sim \mathcal{N}(0,\beta) \Rightarrow y \sim \mathcal{N}(\w\T\x, \beta),
 \end{equation*}
 obtain the maximum likelihood estimate
 \[\hat{y} = \hat{\w}\T\x, \quad \hat{\w} = \argmax_{\w} \frac{1}{2\beta}\sum_{i=1}^{m}(y_i - \w\T \x_i)^2 \]
 for the output.

% Maximum likelihood estimation gives
%\[ \hat{y} = \argmax_{y in Y} P(y|\bx, \w).\]
\section{Mixture Models}
Assume the target variable $\by$ is generated by one of~$K$ linear models $f_{k}(\x, \w_k)$. Let the distribution of the target variable~$\by$ be a mixture of normal distributions
\begin{equation}\label{eq:mixture_models}
p(\by|\x, \bs) = \sum_{k=1}^{K}\pi_k\;\mathcal{N}(\by|\w_k\T\bx,\beta) = \sum_{k=1}^K
\frac{1}{(2\pi\beta_k)^{n/2}} \exp \left(
( -\frac{1}{2\beta_k}(\by - \w_k\T X)^{\top}(\by - \w_k\T X) \right).
\end{equation}
Here~$\bs$ denotes the concatenated vector of parameters:
\[
\bs = [\w_1,\dots,\w_k,\boldsymbol{\pi},\beta]\T,
\]
where $\boldsymbol{\pi} = [\pi_1,\dots,\pi_k]$ are weights of the models,
and $\bB = \beta \bI_m$ is the covariance matrix for~$\by$.

\paragraph{Parameter estimation.}
The goal is to find parameters vector $\hat{\bs}$ which optimizes loglikelihood function for given data set~$D$
\begin{equation}\label{eq:MLE}
\hat{\bs} = \argmax_{\bs}\ln p(\by|\bs), \quad \ln p(\by|\bs) = \sum_{i=1}^{m}\ln\left( \sum_{k=1}^K  \pi_k\mathcal{N}(\by|\w\T_{k}\x_i,\beta) \right).
\end{equation}
%To estimate parameters $\bs$ introduce the matrix
%\[
%Z = \left[\bz_1,\dots,\bz_m|\bz\in\{0,1\}^K\right]
%\]
%of hidden variables. A hidden variable $\bz_i$ indicates which model generates $\mathbf{x}_i$: $\mathbf{x}_i$ is generated by $k$-th model iff $\bz_{ik}=\delta_{ik}$.
%The log-likelihood function for joint distribution of~$\y,Z$ is
%\begin{equation}\label{eq:joint_y_z}
%\ln p(\by,Z|\bs) = \sum_{i=1}^m\sum_{k=1}^K z_{ik} \ln \left( \pi_k \mathcal{N}(y_i|\w\T_{k}\x_i,\beta) \right).
%\end{equation}


To obtain maximum likelihood estimates~\eqref{eq:MLE} for parameter $\bs$ of the model~\eqref{eq:mixture_models}, let us introduce hidden indicator variables
\[Z = \left[\bz_1,\dots,\bz_m\right], \quad z_{ik}\in \{0,1\},\] such that
\[z_{ik} = 1 \Leftrightarrow y_i\sim\mathcal{N}(\w_k\T\x_i, \beta).\]
Then the loglikelihood function $p(\by, Z|X, \bs)$ takes the form
\[p(\by|X, Z, \bs) = \sum_{i=1}^{m}\sum_{k=1}^{K} z_{ik} \left( \ln\pi_k + \ln\mathcal{N}(y_i|\w\T_{k}\x_i,\beta) \right) = \]
\[=\sum_{i=1}^{m}\sum_{k=1}^{K} z_{ik} \left( \ln\pi_k - \frac{1}{2\beta}(y_i-\w\T_{k}\x_i)^2 + \frac{n\ln{\beta}}{2} + \text{const}\right).\]
Since $p(\by, Z|X, \bs)$ depends on random variables $z_{ik}$, instead of $p(\by|X, \bs)$ maximize the expected loglikelihood of the observed data $D$:
\[\mathsf{E}_{Z} [p(\y, Z|X, \bs)] = \sum_{i=1}^{m}\sum_{k=1}^{K} \gamma_{ik} \left( \ln\pi_k - \frac{1}{2\beta}(y_i-\w\T_{k}\x_i)^2 + \frac{n\ln{\beta}}{2}\right), \quad \gamma_{ik} = \mathsf{E}[z_{ik}|\y, X].\]


Finally, apply Expectation-Maximization algorithm to maximize $\mathsf{E}_{Z} [p(\y, Z|X, \bs)]$ updating parameters estimates $\bs^{(r)}$ in two iterative steps.


{\bf E-step}: obtain $\mathsf{E}(Z)$. Let~$\Gamma=[\gamma_{ik}]$ be a matrix of posterior probabilities that $i$-th sample is generated by~$k$-th model. Using Bayesian rule, obtain
\begin{equation}\label{eq:hidden_vars_Estep}
\gamma_{ik}^{(r+1)} = \mathsf{E}(z_{ik}) = p(k|\x_i, \bs^{(r)}) =
    \frac{
        \pi_k\mathcal{N}(y_i|\x_i\T\w^{(r)}_{k}, \beta^{(r)})
        }{
        \sum_{k'=1}^{K}\pi_{k'}\mathcal{N}(y_i|\x_i\T\w^{(r)}_{k}, \beta^{(r)}).
        }
\end{equation}

Define expectations of joint loglikelihood $\ln p(\y, Z|X, \bs)$ with respect to the posteriors distribution~$p(Z|\y,\bs)$
\begin{equation}\label{eq:posterior}
Q^{(r)}(\bs) = \mathsf{E}_Z(\ln p(\y, Z|\bs)) = \sum_{i=1}^m\sum_{k=1}^K\gamma_{ik}^{(r+1)}
    \left(
        \ln\pi^{(r)}_k+\ln\mathcal{N}(y_i|\x_i\T\w^{(r)}_{k},\beta^{(r)})
    \right).
\end{equation}

{\bf M-step:} update parameters $\bs$, maximizing $Q^{(r)}(\bs)$. Maximize function~$Q^{(r)}(\bs)$ with respect to~$\bs$ with~$\Gamma^{(r+1)}$ fixed. First, optimize $\pi_k$, which is constrained as $\sum_{k=1}^K\pi_k=1$. Using Lagrange multipliers, obtain the following estimation
\[
\pi^{(r+1)}_k = \frac{1}{n}\sum_{i=1}^m { \gamma_{ik}^{(r+1)}}.
\]

Next, maximize~$Q^{(r)}$ with respect to~$\w_k$ for~$k$-th model.
With $\pi_k$ fixed maximizing~\eqref{eq:posterior} is equivalent to \
\[
\w^{(r+1)}_k = \argmax_{\w_k}\sum_{i=1}^m -\gamma^{(r+1)}_{ik}
    \left(
        y_i-\w_k\T\x_i
    \right)^2,
\]
\[
\beta^{(r)}_k = \argmax_{\beta}\sum_{i=1}^m\gamma^{(r+1)}_{ik}
    \left(n\ln\beta
        -\frac{1}{\beta}(y_i-\x_i\T\w^{(r+1)}_k)^2
    \right).
\]
%The constant term measures the of $k-$th models, $k\neq i$ into~$Q$.
\section{Mixture of experts}
Suppose that each model $f(\x, \w_k)$ generates a sample $(\x, y)$ with some probability $p(k|\x, \w)$. Then
the following factorization holds
\begin{equation*}\label{eq:mixture_of_experts} p(y|\x, \bs) = \sum_{k=1}^{K} p(y, k| \x, \bs) = \sum_{k=1}^{K} p(k|\x, \bs)p(y|k, \x, \bs)\end{equation*}
 for $p(y|\x, \bs)$. Here $p(k|\x, \bs)$ correspond to weight parameters $\pi_k$ in~\eqref{eq:mixture_models} dependent on the inputs $\x$.
 Assuming normal linear models $f(\x, \w_k)$ or, equivalently, normal  distributions
 $p(y|\bx, \w_k) = \mathcal{N}(y|\w_k\T, \beta),$
 obtain
\begin{equation}\label{eq:mixture_of_experts}p(\by|\x, \bs) = \sum_{k=1}^{K}\pi_k(\x, \bv_k)\mathcal{N}(\by|\w_k\T\bx,\beta), \end{equation}
where
\[\pi_k(\x, \bv_k) = \frac{\exp(\bv_k\T\x)}{\sum_{k'=1}^K \exp(\bv_{k'}\T\x)}.\]
   The difference between mixture of experts model~\eqref{eq:mixture_of_experts} and mixture model~\eqref{eq:mixture_models} in that model weights $\pi_k$ depend on inputs $\x$ in mixture of experts. Similarly, EM-procedure for mixture of experts differs from EM-procedure for mixture models in the way $\gamma_{ik}$ are optimized in M-step.

\begin{algorithm}[ht]
 \KwData{$(\x_i, y_i)$, $i = 1, \dots, m$. Parameters: number of experts $K$.}
 \KwResult{Parameters $\bs$ of the model~\eqref{eq:mixture_of_experts}.}
 Initialize $[\w, \beta, \bv] \equiv \bs = \bs^{(0)}$, $r = 0$\;
 \While{$\bs$ keeps changing}{
  \textbf{E step}: compute hidden variables $\gamma^{(r+1)}_{ik}$, the expectation of the indicator
variables, using~\eqref{eq:hidden_vars_Estep}\;

  \textbf{M step}: find new parameter estimates
  $$ \bv^{(r+1)}_k = \argmax_{\bv} Q^{(r),\bv}_k(\bv), \quad Q^{(r),\bv}_k(\bv) = \sum_{i=1}^m \gamma_{ik}^{(r+1)}\ln\pi_k(\x_i, \bv)$$
  $$ \w^{(r+1)}_k = \argmax_{\w_k} Q^{(r),\w}_k(\w_k), \quad Q^{(r),\w}_k(\w_k) = \sum_{i=1}^m\gamma^{(r+1)}_{ik}
    \left(
        y_i-\w_k\T\x_i
    \right)^2, $$
  $$ \beta^{(r+1)}_k = \argmax_{\beta} Q^{(r),\beta}_k(\beta), \quad Q^{(r),\beta}_k(\beta) =\left(n\ln\beta
        -\frac{1}{\beta}(y_i-\x_i\T\w^{(r+1)}_k)^2
    \right)
    \;$$
 }
 \caption{EM-algorithm for mixture of experts.}
\end{algorithm}


\end{document} 