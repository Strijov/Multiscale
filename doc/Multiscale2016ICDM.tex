\documentclass[conference]{IEEEtran}
%\usepackage{a4wide}
\usepackage{NotationStyle}
\usepackage[ruled,noend,noline,slide]{algorithm2e}
\usepackage{multicol, multirow}
%\usepackage[noend]{algorithmic}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,mathrsfs,mathtext}
\usepackage{subfig}
\usepackage{graphics,graphicx,epsfig}
\usepackage{epstopdf}
\usepackage{fancybox,fancyhdr}
\usepackage{enumerate}
\usepackage{array}
\usepackage{color, soul}
%\usepackage{longtable} % longtable doesn't work with two columns
\usepackage{supertabular}
\usepackage[normalem]{ulem}
\usepackage{arydshln}
%\usepackage{accents} % for the sake of doublehat!
%\newlength{\dhatheight}
%\newcommand{\doublehat}[1]{%
%    \settoheight{\dhatheight}{\ensuremath{\hat{#1}}}%
%    \addtolength{\dhatheight}{-0.35ex}%
%    \hat{\vphantom{\rule{1pt}{\dhatheight}}%
%    \smash{\hat{#1}}}}


%\makeatletter
%\let\oldlt\longtable
%\let\endoldlt\endlongtable
%\def\longtable{\@ifnextchar[\longtable@i \longtable@ii}
%\def\longtable@i[#1]{\begin{figure}[t]
%\onecolumn
%\begin{minipage}{0.5\textwidth}
%\oldlt[#1]
%}
%\def\longtable@ii{\begin{figure}[t]
%\onecolumn
%\begin{minipage}{0.5\textwidth}
%\oldlt
%}
%\def\endlongtable{\endoldlt
%\end{minipage}
%\twocolumn
%\end{figure}}
%\makeatother

\setlength\dashlinedash{0.2pt}
\setlength\dashlinegap{4.5pt}


\newcommand{\argmax}{\mathop{\rm arg\,max}\limits}
\graphicspath{ {../code/fig/} {fig/} {../code/}}

\renewcommand{\baselinestretch}{1.3}
\begin{document}
\title{Feature generation for multiscale time series forecasting}



%Radoslav Neychev, Eric Gaussier and Vadim Strijov}
\date{Technical report (pre-draft)}
\maketitle

\begin{abstract}
The paper presents a framework for the massive multiscale time series forecast. We propose a method of constructing efficient feature description for the corresponding regression problem. The method involves feature generation and dimensionality reduction procedures. Generated features include historical information about the target time series as well as other available time series, local transformations and  multiscale features. We apply several forecasting algorithms to the resulting regression problem and investigate the quality of the forecasts for various horizon values.
\end{abstract}



\section{Introduction}
We focus on the problem of forecasting behavior of a device within the concept of Internet of Things. The device at question is monitored by a set of sensors, which produces large amount of multi-scale time series during its lifespan. These time series have various time scales since distinct sensors produce observations with various frequencies from milliseconds to weeks.  The main goal is to predict the observations of a device in a given time range.

We assume that the sampling rate of each time series is fixed and each time series has its own forecast horizon. The problem of multi-scale analysis arises in such applications as weather prediction, medical diagnosis and monitoring various sensor time series~\cite{Costa2008, Ahmed2012, Cortez2012, Ferreira2006}. Motivation for multi-scale analysis comes from the assumption that the behaviour of complex signals may be governed by  essentially different processes at various time scales. Thus, the time series should be modeled separately at each scale. This approach is used in time series classification, prediction and fault detection~\cite{Cui2016, Cortez2012, Aldrich2013}. Regardless of the goal of multi-scale analysis, it includes sequential averaging of the time series to obtain more coarse-scaled time series~\cite{Wu2013}, or, more rarely, differencing the time series for a more detailed, fine-scaled version of the time series~\cite{Jiang2011}. Averaging and differencing, which is equivalent to application of Haar's wavelet transform~\cite{Jiang2011}, may be replaced by any other pair of low and high pass wavelet filters~\cite{Chen2004} or convolution operation with some kernel function~\cite{Vespier2012}. Using multi-scale approach in time series prediction usually involves determining optimal scales~\cite{Vespier2012, Ahmed2012}, decomposition of time series into separately forecasted components and combination of the obtained forecasts.

The problem gets more complex when the task is to forecast multiple time series, which are characterized with different scales and ranges. Forecasting time series separately might lead to loss of valuable information. On the other hand, forecasting the time series simultaneously might lead to increased errors since not all the time series in the given set necessarily depend on the others. In this paper we propose a novel framework for multiscale time series forecasting, which is based on regression-based forecasts. The goal is to obtain forecasts of all time series from the given set simultaneously. Adopting this approach we endeavour to profit as much as possible from the interconnections between the time series of the set while keeping the decrease in forecasting quality for the independent time series reasonably small. Within the proposed framework the time series of various scales are combined into are single regression problem. The forecasts viewed as target variables of the regression problem, where feature description contains local history of the time series as well as various derivations. We describe the steps of creating feature description to this problem: composition of design matrix, feature generation and selection. Note that the problem of model selection rests beyond the scope of the paper. To illustrate the proposed framework in application to the multiscale data set~\cite{EnergyWeatherData} we use several widely used regression models~\cite{Yu2016, Kane2014, Trafalis2000, Navarrete2015, Taylor2009, Qiu2014}. The following section provides a brief overview of these methods and provide the motivation to use them.

\section{Related work}
 Along with generic methods of time series forecasting, such as Autoregressive Moving Average Models (ARMA), Autoregressive Integrated Moving Average Models (ARIMA), authors report high predictive performance of the methods, originally developed for classification or regression, applied to forecast time series~\cite{Yu2016, Kane2014, Trafalis2000, Navarrete2015, Busseti2012, Taylor2009}.  Here the input variables are the delayed observations  of the time series, and the output is the forecasted value of time series. However, the authors of~\cite{Navarrete2015} show that this prediction framework suffers from systematic error that does not converge to zero as the sample size increases, and  ensure error convergence applying cubic spline approximation to noisy data, which yields much lower RMSE in case of noisy data.

 To extend this one-step-ahead forecasting scheme to the case of multiple predictions, one may use iterative, direct or multiple output strategies~\cite{Bao2014}. Within the iterative strategy, one-step-ahead forecasts are computed recursively, with the newly predicted values of the time series used as the actual future records. A less prone to error accumulation, though more time consuming method is the direct strategy, which involves estimation of $h$ models to predict $h$ future values of the time series~\cite{Zhang2013}. Finally, the multiple input multiple output (MIMO) strategy allows to obtain $h$ prediction with at one step.  The paper~\cite{Bao2014} compares different strategies of multi-step-ahead prediction in SVR-based forecasting: direct, iterative and multiple output. Regardless of the horizon values, direct and MIMO strategies consistently achieve more accurate forecasts, than the iterative strategy, with MIMO being most accurate in most cases.


To demonstrate the application of the proposed framework of time series forecasting, we utilize Multivariate Linear Regression (MLR) as the naive approach, as well as three more complex models: Random Forests (RF)~\cite{Yu2016, Kane2014}, Support Vector Regression (SVR)~\cite{Trafalis2000, Navarrete2015, Hao2006} and artificial neural networks (ANN)~\cite{Busseti2012, Taylor2009}.  Random Forests combine decision trees with randomly generated nodes to increase the accuracy of classification or regression~\cite{Criminisi2011}. In case of regression trees, each node of the tree splits the input space into two subspaces and each leaf specifies a distinct regression model, which is used for prediction if the input is found in the corresponding region of the input space. Predictions of the trees in the forest are averaged, or, for the probabilistic random forest, the probabilities of the outputs are averaged. The advantage of random forests is their efficiency in case of highly dimensional data due to the randomness incorporated into selecting informative features. Since random forests are essentially ensembles of weak learners, they enjoy high generalization ability, associated with boosting algorithms.
 Similarly, the formulation of optimization problem within support vector regression promotes its robustness in case of highly dimensional data. The authors of~\cite{Trafalis2000, Hao2006} reported high predictive performance of SVR applied to time series forecasting. In case of SVR, MIMO strategy is based on multivariate SVR~\cite{PerezCruz2002}. Finally, artificial neural networks attract researches and practitioners from various domains~\cite{Taylor2009, Qiu2014}. One of the reasons for that is the ability of ANNs to model complex relationships between the input data in such fashion that does not require direct feature engineering. For more suggestions on how to combine these forecasting methods~\cite{Qiu2014, Grover2015} or use them in the multi-scale fashion we refer the reader to~\cite{Chen2004, Zhu2012, Cui2016, Bai2015, Ferrari2012}.


\section{Problem statement}
Consider a large set of time series~$\fD=\{\bs^{(q)}| \; q = 1,\dots, {Q}\}$, where each real-valued time series~$\bs$
\[ \bs = [s_1, \dots, s_i, \dots, s_{T}], ~~ s_i = s(t_i),\quad 0 \leq t_i \leq t_{\max}\]
is a sequence of observations $s_i = s(t_i)$ of some real-valued signal $s(t)$.
Each time series $\bs^{(q)}$ has its own sampling rate $1/\tau^{(q)}$:
\[t_i^{(q)} = {i}\cdot\tau^{(q)}.\]

 The task is to obtain forecasts $\hat{s}(t_i)$ of $\bs \in \fD$ for  $\dtr <  t_i \leq T_{\max} + \dtr$, given the set  $\fD$ (see Fig.~\ref{fg:online_frc}). The forecasts $\hat{\bs}$ should minimise symmetric mean absolute percentage error:
 \begin{equation}\label{eq:MAPE}
 SMAPE(\bs, \hat{\bs}) = \frac{1}{r}\sum_{i = 1}^{r} \frac{2|s_i - \hat{s}_i|}{|s_i + \hat{s}_i|}. \end{equation}
 Here and throughout this paper we assume that each time series are standardized.


\begin{figure}[!ht]
\def\svgwidth{0.4\textwidth}
\centering
%\subfloat[]{
\input{fig/online_forecasting_paradigm.eps_tex}
%\label{fg:online_frc}} \\
%\def\svgwidth{0.4\textwidth}
%\centering
%\subfloat[]{\input{fig/draw_object.eps_tex}\label{fg:draw}} \\
%\def\svgwidth{0.4\textwidth}
%\centering\subfloat[]{\input{fig/design_matrix_generation.eps_tex}\label{fg:design}}
\caption{Illustration of the procedure of design matrix composition.}\label{fg:online_frc}
\end{figure}


\begin{figure}[!ht]
\def\svgwidth{0.4\textwidth}
\centering
\input{fig/draw_object.eps_tex}
\caption{Illustration of the procedure of design matrix composition.}\label{fg:draw}
\end{figure}


\begin{figure}
\def\svgwidth{0.4\textwidth}
\centering \subfloat[]
%{\includegraphics[width=0.4\textwidth]{forecasting_model.png}
{\input{fig/forecasting_model.eps_tex}
\label{fg:forecasting}
}
\caption{Forecasting as regression problem.}
\end{figure}




\subsection{Design matrix}

 We consider the forecasting problem as the multivariate regression problem, where target variables are the vectors of lagged values $s(t_i)$ of all the time series $\bs \in \fD$.

Let $\bx^{*}$ denote rows of the design matrix~$\bX^*$ for the regression problem. Each vector~$\bx^{*} = [\bx| \by]$ collects all the time series over the time period~$\dtp$ (Fig.~\ref{fg:draw}), which stands for the local \emph{prehistory}. The vector $\bx^{*}$ includes samples from previous history of time series from $\fD$ as well as any derivatives or \emph{generated features}. We describe the types of generated features in Section~\ref{sc:feature_generation}.

The design matrix~$\bX^*$ for the multiscale autoregressive problem statement is constructed  as follows. Let $\bs^{(q)}_i$ denote the~$i$-th segment of the time series $\bs^{(q)}$
\begin{equation}\label{eq:segment}
[\bx^{(q)}_i | \by^{(q)}_i] = \end{equation}
\[ \underbrace{s^{(q)}(t_i-\dtr-\dtp),\dots,}_{\bx^{(q)}_i} \underbrace{s^{(q)}(t_i-\dtr),\dots,s^{(q)}(t_i))}_{\by^{(q)}_i}], \]

where~$s^{(q)}(t)$ is an element of time series~$\bs^{(q)}$. To construct the design matrix, select $t_i$, $i = 1, \dots, m$ from $\Gs = \{t_1, \dots, t_T\}$ such that segments $\bs_i = [\x_i|\y_i]$ cover time series $\bs$
%\begin{equation}\label{eq:strategy2} \{s(t_{1}), \dots, s(t_{\max})\} = \bigcup_{i=1}^{q-1} \{s(t_i-\Delta t_\text{r}),\dots,s(t_i)\}\end{equation}
without intersection in target parts  $\y_i$:
\begin{equation}\label{eq:strategy2} |t_{i+1} - t_i| > \dtr.
\end{equation}
Following~\eqref{eq:segment} and~\eqref{eq:strategy2}, extract segments $[\bx^{(q)}_i | \by^{(q)}_i]$, $i = 1, \dots, m$ from all time series $\bs^{(q)} \in \fD$ and form the matrix
 \begin{equation}\label{eq:design_matrix}
\bX^*= \left[
\begin{array}{c|c}
\underset{1{\times}n}{\x} & \underset{1{\times}r}{{\by}}  \\
\hline
 \underset{m{\times}n}\bX & \underset{m{\times}r}\bY  \\
 %\hline
 \end{array}
\right] = \end{equation}
\[ \left[
\begin{array}{lll|lll}
%\bx^{(1)} & \dots & \bx^{(Q)} & \by^{(1)} &  \dots & {\by}^{(Q)}   \\
%\hline
\bx_m^{(1)}  & \dots & \bx_m^{(Q)} & \by_m^{(1)} &  \dots & \by_m^{(Q)}   \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots  \\
\bx_1^{(1)} & \dots & \bx_1^{(Q)} & \by_1^{(1)}  & \dots & \by_1^{(Q)}   \\


\end{array}
\right]. \]

Denote a row from the pair~$\bY,\bX$ as $\by,\bx$ and call these vectors the target and the features.

%\begin{figure}[!ht]
%\def\svgwidth{0.4\textwidth}
%\centering\input{fig/retrospective_validation.eps_tex}
%\caption{\hl{Retrospective forecast includes most recent samples in data set.}\label{fg:retrospective}}
%\end{figure}

Now we are able the regression problem as follows:
\begin{equation}\label{eq:regression_problem}
\hat{\y} = f(\x, \hat{\w}), ~\hat{\w} = \argmin_{\hat{\w}}S\bigl(\bw|\fx(\bw,\bx),\by\bigr). %\text{~where~}
\end{equation}
Here the error function is given by $SMAPE$~\eqref{eq:MAPE} for each segment $[\x_i | \by_i]$, averaged over all segments $i = 1, \dots, m$ in the test set:
\[ S\bigl(\bw|\fx(\bw,\bx),\by\bigr) = \frac{r}{m}\sum_{i=1}^{m} SMAPE(\y_i, f(\x_i, \w)).\]

\section{Feature generation}\label{sc:feature_generation}
Denote the generated feature vector as~$\vf$. This vector consists of concatenated row-vectors~$\vf=[\vf^{(1)},\dots,\vf^{(Q)}]$, which corresponds to time series local histories ~$\bs=[\bs^{(1)},\dots,\bs^{(Q)}]$, modified with set of transformations~$G$. The elements~$g:\bs\to\vf$ of this set are listed below. The augmented feature set $\vf$ includes
\begin{enumerate}[1)]
\item the local history of all time series themselves,
\item transformations (non-parametric and parametric) of local history,
\item parameters of the local models,
\item distances to the centroids of local clusters.
\end{enumerate}



\subsection{Transformations of local history}\label{sc:Transform}
We use non-parametric and parametric functions to generate features. The purpose of this block of features is to introduce nonlinearities into the feature space of regression problem~\eqref{eq:regression_problem}.

The parametric procedure involves two optimization problems. The first one fixes the vector~$\hat\bb$, collected over all the primitive functions~$g=g(\bb,s) \in G$, which generate features~$\vf$:
\[
\hat\bw = \arg\min_{\bw} S\bigl(\bw|\fx(\bw,\vf),\by\bigr),\quad
\text{where}
\quad \vf = g(\hat\bb,\bs).
\]
The second one optimizes the transformation parameters~$\hat\bb$ given the obtained model parameters~$\bw$
\[
\hat\bb = \arg\min_{\bb} S\bigl(\bb|\fx(\hat\bw,\vf),\by\bigr).
\]
This parametric feature generation procedure repeats these problems until vectors~$\hat\bw,\hat\bb$ converge. The initial values of the parameters~$\bb$ are assigned empirically.

\subsection{Convolutions, statistics and parameters of local history}\label{sc:Conv}
This block of feature generation functions includes convolutions, time averaging and differencing, and basic statistics of each time series, such as mean and standard deviation, minimum and maximum of the input $\bx$. The features from these part can be seen as applying Haar's wavelet transform to each segment~\cite{Jiang2011}. Motivation for this comes from assuming the multi-scale nature of the time series: complex signals may be governed by essentially different processes at various time scales. Averaging of the time series allows to obtain more coarse-scaled time series, while differencing the time series provides a more detailed, fine-scaled version of the time series.


\subsection{Parameters of local history forecast}
For the time series~$\bs$ construct the Hankel matrix~\cite{Motrenko2016} with a~period~$k$ and shift~$p$, so~that for~$\bs = [s_1,\dots,s_T]$ the matrix
\[
\bH^* =
\left[ \begin{array}{c|cc}
s_T  & \dots & s_{T-k+1} \\
\vdots & \ddots & \vdots \\
s_{k+p} & \dots & s_{1+p}\\
s_k & \dots & s_1 \\
\end{array}
\right],
\text{~where~} 1\geqslant p \geqslant k .
\]
Reconstruct the regression to the first column of the matrix~$\bH^*=[\bh, \bH]$ and denote its least square parameters as the feature vector
\begin{equation}\label{eq:SSA}
\boldsymbol{\phi}^{(q)} = \arg\min \| \bh-\bH \boldsymbol{\phi}\|_2^2.
\end{equation}
For the time series~$\bs^{(q)}$, $q=1,\dots, Q$ use the parameters $\boldsymbol{\phi}^{(q)}$ as the features.

\subsection{Distances to centroids of local clusters}\label{sc:centroids}
This procedure applies the kernel trick to the time series. For given local history time series~$\bx_i^{(q)}$, $q=1,\dots, Q$ compute $k$-means centroids~$\bc_p^{(q)}$, $p = 1, \dots, P$.  With the selected $k$-means distance function~$\rho$ construct the feature vector
\begin{equation}\label{eq:centroids}
\vf_i^{(q)} = [\rho(\bc_1^{(q)},\bs_i^{(q)}),\dots,\rho(\bc^{(q)}_P,\bs_i^{(q)})] \in \R_+^P.
\end{equation}
This $k$-means of another clustering procedure may use internal parameters, so that there are no parameters to be included to the feature vector or to the forecasting model.

\begin{algorithm}[!h]
%\DontPrintSemicolon
\KwData{Object-feature matrix $\bX^{*} \in \mathbb{R}^{m\times(n+r)}$. Train to test ratio $\alpha \in [0, 1]$.}
 \KwResult{Train and test, $\bX^{*}_{\text{train}}$, $\bX^{*}_{\text{test}}$.}
 Set train set and test set sizes:

 $ \quad m_{\text{train}} = \lfloor\alpha\cdot m\rfloor, \quad m_{\text{test}} = m - m_{\text{train}} $ \; %(1-\alpha)(m-1)
 Decompose matrix $\bX^{*}$ into train and test matrices $\bX^{*}_{\text{train}}$, $\bX^{*}_{\text{test}}$:
 \[\bX^{*} = \left[\begin{array}{c|c}
 \underset{m_{\text{test}}{\times}n}{\bX_{\text{test}}} & \underset{m_{\text{test}}{\times}r}{\bY_{\text{test}}}  \\
 \hdashline
 \underset{m_{\text{train}}{\times}n}{\bX_{\text{train}}}  & \underset{m_{\text{train}}{\times}r}{\bY_{\text{train}}}
 \end{array}\right]
 \]
  \caption{Initial train-test splitting procedure.}\label{alg:train_test}
\end{algorithm}

\section{Testing procedure}\label{sc:testing}
The algorithm below describes the procedure used to evaluate the forecasting errors within the proposed framework given the model $\fx$,  data matrix $\bX^{*} \in \mathbb{R}^{m\times(n+r)}$ and fixed parameters train to test ratio $\alpha$, minimal sample (test) size $m_{\min}$. This procedure involves creation of design matrix~\eqref{eq:design_matrix}, generation of augmented feature description $\vf$ and, since it is likely to be redundant, dimensionality reduction. Here we use principal component analysis (PCA) and nonlinear PCA~\cite{MatthiasScholz2007}.
\begin{enumerate}[1)]
\item Create design matrix $\bX^{*}$ according to~\eqref{eq:design_matrix} from $\fD$.
\item Split matrix $\bX^{*}$ into train and test matrices $\bX^{*}_{\text{train}}$ and $\bX^{*}_{\text{test}}$
according to the train-test splitting procedure~\ref{alg:train_test}\;
\item Augment $\bX^{*}_{\text{train}}$ with generated features $\vf$\;
\item Reduce dimensionality of $\bX^{*}_{\text{train}}$ \;
\item Optimize hyper parameters of the model $\fx$, using $\bX^{*}_{\text{train}}$\;
\item
For $k$ in $\{1, \dots,  m_{\text{test}} - m_{\min}\}$ repeat:
\begin{itemize}
\item define $\bX^{*}_{\text{train}, i}$ as $(i+1)$-th to $(i+ m_{\min} + 1)$-th rows of $\bX^{*}_{\text{test}}$ and
  $\bx^{*}_{\text{val}, i}$ as the $i$-th row of $\bX^{*}_{\text{test}}$\;

 \[\bX^{*}_{\text{test}} = \left[\begin{array}{c|c}
 \dots & \dots \\
 \hline
 \underset{1{\times}n}{\bx_{\text{val}, i}} & \underset{1{\times}r}{\by_{\text{val}, i}}  \\
 \hdashline
 \underset{m_{\min}{\times}n}{\bX_{\text{train}, i}}  & \underset{m_{\min}{\times}r}{\bY_{\text{train}, i}} \\
 \hline
 \dots & \dots \\
 \end{array}\right]
 \]
\item apply feature transformation to $\bX^{*}_{\text{train}, i}$, $\bX^{*}_{\text{val}, i}$\;
\item  train forecasting model $\fx(\x, \hat{\w}_i)$, using $\bX^{*}_{\text{train}, i}$\;
\item  obtain vector of residuals $\veps  = \by_{\text{val}, i} - \fx(\bx_{\text{val}, i}, \hat{\w}_i)$\;
\item  compute forecasting quality:
  \[ {SMAPE}(i)  = \frac{1}{r} \sum_{t=1}^{r} \frac{2|\varepsilon_{t}|}{|2(y_{\text{val}, i})_t - \varepsilon_{t}|};\]
\end{itemize}
\item
  Return $SMAPE$, averaged over data splits:
  \[ \text{Error}  = \frac{1}{m_{\text{test}} - m_{\min}} \sum_{i=1}^{m_{\text{test}} - m_{\min}} {SMAPE}(i).\]
\end{enumerate}
The models that we use are listed in the table~\ref{tb:regr_mdl} along with the optimized hyper parameters.


\section{Computational experiment}
This section presents the results of computational validation of the proposed framework.
\begin{table}\caption{Regression models.}\label{tb:regr_mdl}
\begin{tabular}{|p{4cm}|p{4cm}|}
\hline
Model name & Hyper parameters \\
\hline
Baseline method: $\hat{s}_i = s_{i-1}$ & None \\
\hline
 Multivariate linear regression (MLR) with $l_2$-regularization & Regularization coefficient: 2 \\
 \hline
%\[f(\bx, \bW) = \bx\bW , ~~ \hat{\bW} = \argmin_{\bW} S(\bW) + \lambda||\bW||_2^2, \]
Support vector regression with multiple output (MSVR) & Kernel type: RBF, $p_1$: 2, $p_2$: 0, $\gamma$: 0.5, $\lambda$: 4 \\
%where the regression function also has the linear form $f(\bX, \{\bW, \bb\}) = \bx\bW + \bb$, but the optimization problem is more complex:
%    \[ \hat{\bW} = \argmin_{\bW,\;\vx,\;\vx^{*}} \frac{1}{2}||\bW||_2^2 + C
%    \sum_{i, j}(\xi_{ij} + \xi_{ij}^{*}), \text{~subject to~}\]
%    \[ \begin{cases} -\veps - \vx^{*}_i \leq \by_i - \bx_i\bW  - \bb \leq \veps + \vx_i, \\
%     \xi_{ij}\xi^{*}_{ij} > 0.
%    \end{cases}\]
\hline
Artificial neural network (ANN). Feed-forward ANN with single hidden layer  &  Hidden layers size: 25 \\
\hline Random forest (RF) & Number of trees: 25 , number of variables for each decision split: 48\\
\hline
\end{tabular}
\end{table}
\subsection{Datasets}
The computational experiments demonstrated in this section are based on the Energy-Weather data set~\cite{EnergyWeatherData}. The dataset consists of the Polish electricity load time series and weather time series in Warsaw (Longtitude: 21.25, Latitude: 52.30, Elevation: 94). Energy time series contain hourly records (total of 52512 observations), while weather time series were measured daily and contain 2188 observations. The multiscale time series correspond to the period of 1999 to 2004. The results observed on this data set  are illustrative of the proposed framework since the data set contains the time series that are both multiscale and have various nature.

The Energy-Weather data set was used to generate several data sets with artificial inserted missing values. The ratios of missing data are 0.01, 0.03, 0.05 and 0.1.

%\item Accelerometry dataset. This dataset consists of accelerometry time series from the Human Activity Sensing Consortium~\cite{HASCdata}. Each time series in the datasets is a sequence of acceleration records. The time series were recorded for 120 seconds while a subject performed a sequence of activities: stay, walk, jog, skip, stair up or stair down. The sampling rate varies between 10 and 100Hz, but stays constant for each time series.
%\item Financial dataset~\cite{NNcompetition}.  The dataset A (final, complete) from the 2006/07 Forecasting Competition for Neural Networks \& Computational Intelligence. The dataset contains 111 monthly time series drawn from homogeneous population of empirical business time series.


\subsection{Experimental results}

Fig.~\ref{fg:target_data} displays a range of target variables $\by$ generated for the Energy-Weather data set.

Fig.~\ref{fg:pca_frc} demonstrate the examples of forecasts of individual time series, obtained within the proposed framework. Here the design matrix was augmented with the generated features and PCA was applied to select a subset of features.

Table~\ref{fg:feature_selection_res} lists forecasting errors for the proposed feature generation strategies applied to time series from the original Energy-Weather data set. The errors were computed following the testing procedure, detailed in the section~\ref{sc:testing}. After the multiple forecasts were obtained, $SMAPE$ was computed for each time series separately. Multirows labeled ``Features'' unite results of each model for the particular feature set. The tested options are:
\begin{itemize}
\item ``History'' corresponds to the standard regression-based forecast with no additional features.
\item Each multirow from ``SSA'' to ``NW''  corresponds to a particular feature set added to historical features separately from other generated features. Here ``SSA'' stands for parameters of local approximation~\eqref{eq:SSA}, ``Cubic'' stands for coefficients of cubic spline approximation, ``Conv''~--- for multiscale features and statistics listed in section~\label{sc:Conv} and ``Centroids''~--- for the feature set defined by~\eqref{eq:centroids}.
\item ``All'' stands for all feature generation strategies applied to the dataset, with no feature selection.
\item  ``PCA'' and ``NPCA'' present the results of applying PCA and NPCA after all generation strategies were used.
\end{itemize}
The top row of Table~\ref{fg:feature_selection_res} lists results of the baseline method: for each time series the next forecasted value is predicted with the most recent observed value. As can be seen from the table, the forecasting quality generally improves for all the time series, even though the weather data is unlikely to depend on the energy consumption and the multiple all-on-all regression could lead to increased errors. The errors of the data sets with missing values increase as the ratio of missing data gets higher but the general pattern does not change.  According to our results the feature sets differ very slightly among feature generation strategies and generally demonstrate poorer performance than the historical features, though this is not always the case. We also note that there is no single best or worst combination of model, feature generation and feature selection strategy for all the time series. This motivates us to direct our further research to ensembles of learners.

\begin{table*}
\begin{tabular}{|p{0.8cm}|p{0.8cm}||c|c||c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
SMAPE & Data & \multicolumn{2}{c||}{Energy} & \multicolumn{2}{c|}{Max T.} & \multicolumn{2}{c|}{Min T.}  & \multicolumn{2}{c|}{Precipitation}  & \multicolumn{2}{c|}{Wind} &  \multicolumn{2}{c|}{Humidity} &  \multicolumn{2}{c|}{Solar} \\
\hline
Features & Models & test & train & test &  train & test &  train & test &  train & test &  train & test &  train & test &  train\\
\hline
\hline
History & Baseline & 0.1948  &  0.2095 &   0.1040  &  0.1351 & 0.1047  &  0.1141  &  1.2034  &  1.2908 & 0.4581  &  0.4600  &  0.1803  &  0.1918 & 0.4641  &  0.5184\\
\hline
\hline
\multirow{4}{*}{History} &MLR &   0.130 &    0.143 &    0.090 &    1.265 &    0.395 &    0.239 &    0.673 &    0.108 &    0.079 &    0.057 &    1.164 &    0.358 &    0.168 &    1.247\\
\cline{2-16}
 &MSVR &   0.280 &    0.377 &    0.227 &    1.243 &    0.415 &    0.369 &    0.780 &    0.025 &    0.055 &    0.033 &    1.013 &    0.114 &    0.059 &    0.318\\
\cline{2-16}
 &RF &   0.137 &    0.180 &    0.111 &    1.306 &    0.417 &    0.257 &    0.473 &    0.047 &    0.043 &    0.030 &    1.031 &    0.214 &    0.102 &    0.283\\
\cline{2-16}
 &ANN &   0.157 &    0.180 &    0.102 &    2.050 &    0.596 &    0.281 &    1.526 &    0.089 &    0.102 &    0.076 &    1.418 &    0.358 &    0.146 &    0.661\\
\cline{2-16}
\hline
\hline
\multirow{4}{*}{SSA} &MLR &   0.130 &    0.144 &    0.090 &    1.349 &    0.394 &    0.239 &    0.929 &    0.108 &    0.079 &    0.057 &    1.161 &    0.358 &    0.169 &    1.117\\
\cline{2-16}
 &MSVR &   0.317 &    0.413 &    0.242 &    1.237 &    0.422 &    0.397 &    0.837 &    0.029 &    0.067 &    0.040 &    1.016 &    0.113 &    0.060 &    0.351\\
\cline{2-16}
 &RF &   0.137 &    0.181 &    0.112 &    1.298 &    0.438 &    0.250 &    0.465 &    0.047 &    0.044 &    0.031 &    1.013 &    0.212 &    0.102 &    0.280\\
\cline{2-16}
 &ANN &   0.171 &    0.209 &    0.163 &    4.207 &    0.464 &    0.257 &    1.077 &    0.120 &    0.100 &    0.087 &    2.289 &    0.380 &    0.190 &    0.546\\
\cline{2-16}
\hline
\hline
\multirow{4}{*}{Cubic} &MLR &   0.130 &    0.143 &    0.090 &    1.316 &    0.395 &    0.239 &    0.657 &    0.108 &    0.079 &    0.057 &    1.164 &    0.358 &    0.168 &    0.668\\
\cline{2-16}
 &MSVR &   0.280 &    0.378 &    0.227 &    1.243 &    0.415 &    0.369 &    0.781 &    0.025 &    0.055 &    0.033 &    1.015 &    0.114 &    0.059 &    0.318\\
\cline{2-16}
 &RF &   0.137 &    0.188 &    0.112 &    1.289 &    0.427 &    0.259 &    0.489 &    0.047 &    0.045 &    0.031 &    1.017 &    0.216 &    0.105 &    0.288\\
\cline{2-16}
 &ANN &   0.162 &    0.232 &    0.125 &    2.905 &    0.599 &    0.337 &    1.171 &    0.103 &    0.094 &    0.062 &    6.119 &    0.416 &    0.146 &    0.523\\
\cline{2-16}
\hline
\hline
\multirow{4}{*}{Conv} &MLR &   0.126 &    0.146 &    0.090 &    1.457 &    0.397 &    0.241 &    0.762 &    0.103 &    0.078 &    0.057 &    1.162 &    0.355 &    0.168 &    0.637\\
\cline{2-16}
 &MSVR &   0.298 &    0.395 &    0.234 &    1.242 &    0.417 &    0.383 &    0.811 &    0.026 &    0.058 &    0.035 &    1.068 &    0.113 &    0.060 &    0.331\\
\cline{2-16}
 &RF &   0.139 &    0.211 &    0.124 &    1.303 &    0.432 &    0.265 &    0.480 &    0.049 &    0.045 &    0.032 &    1.013 &    0.219 &    0.106 &    0.274\\
\cline{2-16}
 &ANN &   0.183 &    0.205 &    0.205 &    2.353 &    0.562 &    0.303 &    1.586 &    0.114 &    0.110 &    0.107 &    1.646 &    0.382 &    0.168 &    1.507\\
\cline{2-16}
\hline
\hline
\multirow{4}{*}{Centroids} &MLR &   0.136 &    0.164 &    0.108 &    1.356 &    0.420 &    0.260 &    0.652 &    0.097 &    0.075 &    0.052 &    1.213 &    0.346 &    0.163 &    0.892\\
\cline{2-16}
 &MSVR &   0.327 &    0.424 &    0.247 &    1.236 &    0.424 &    0.408 &    0.849 &    0.030 &    0.069 &    0.042 &    0.974 &    0.113 &    0.061 &    0.356\\
\cline{2-16}
 &RF &   0.137 &    0.181 &    0.109 &    1.295 &    0.424 &    0.261 &    0.498 &    0.047 &    0.043 &    0.030 &    1.021 &    0.210 &    0.105 &    0.285\\
\cline{2-16}
 &ANN &   0.189 &    0.277 &    0.118 &    2.960 &    0.464 &    0.306 &    0.930 &    0.122 &    0.090 &    0.079 &    1.551 &    0.356 &    0.187 &    0.481\\
\cline{2-16}
\hline
\hline
\multirow{4}{*}{NW} &MLR &   0.130 &    0.149 &    0.094 &    1.322 &    0.411 &    0.238 &    0.672 &    0.114 &    0.084 &    0.063 &    1.194 &    0.377 &    0.184 &    0.619\\
\cline{2-16}
 &MSVR &   0.293 &    0.383 &    0.228 &    1.247 &    0.419 &    0.375 &    0.796 &    0.022 &    0.048 &    0.030 &    0.929 &    0.130 &    0.069 &    0.293\\
\cline{2-16}
 &RF &   0.140 &    0.207 &    0.129 &    1.304 &    0.431 &    0.275 &    0.483 &    0.048 &    0.044 &    0.032 &    1.026 &    0.219 &    0.106 &    0.285\\
\cline{2-16}
 &ANN &   0.186 &    0.193 &    0.115 &    6.417 &    0.494 &    0.275 &    1.033 &    0.124 &    0.097 &    0.074 &    1.358 &    0.427 &    0.194 &    1.264\\
\cline{2-16}
\hline
\hline
\multirow{4}{*}{All} &MLR &   0.132 &    0.140 &    0.100 &    1.410 &    0.418 &    0.244 &    1.514 &    0.105 &    0.082 &    0.062 &    1.192 &    0.369 &    0.182 &    0.768\\
\cline{2-16}
 &MSVR &   0.323 &    0.415 &    0.242 &    1.238 &    0.424 &    0.399 &    0.845 &    0.027 &    0.064 &    0.038 &    1.013 &    0.117 &    0.061 &    0.346\\
\cline{2-16}
 &RF &   0.139 &    0.220 &    0.134 &    1.292 &    0.439 &    0.294 &    0.495 &    0.048 &    0.046 &    0.033 &    1.016 &    0.221 &    0.106 &    0.270\\
\cline{2-16}
 &ANN &   0.208 &    0.251 &    0.233 &    5.489 &    0.511 &    0.323 &    1.063 &    0.145 &    0.110 &    0.108 &    2.916 &    0.359 &    0.176 &    2.007\\
\cline{2-16}
\hline
\hline
\multirow{4}{*}{PCA} &MLR &   0.133 &    0.159 &    0.110 &    1.272 &    0.422 &    0.242 &    4.674 &    0.115 &    0.091 &    0.068 &    1.234 &    0.383 &    0.189 &    0.692\\
\cline{2-16}
 &MSVR &   0.321 &    0.412 &    0.241 &    1.238 &    0.423 &    0.397 &    0.841 &    0.027 &    0.063 &    0.037 &    1.030 &    0.118 &    0.061 &    0.345\\
\cline{2-16}
 &RF &   0.185 &    0.236 &    0.155 &    1.298 &    0.453 &    0.311 &    0.603 &    0.062 &    0.053 &    0.038 &    1.022 &    0.225 &    0.113 &    0.299\\
\cline{2-16}
 &ANN &   0.220 &    0.256 &    0.169 &    10.457 &    0.506 &    0.357 &    2.979 &    0.150 &    0.155 &    0.108 &    1.468 &    0.414 &    0.220 &    2.433\\
\cline{2-16}
\hline
\hline
\multirow{4}{*}{NPCA} &MLR &   0.133 &    0.159 &    0.110 &    1.272 &    0.422 &    0.242 &    4.674 &    0.115 &    0.091 &    0.068 &    1.234 &    0.383 &    0.189 &    0.692\\
\cline{2-16}
 &MSVR &   0.321 &    0.412 &    0.241 &    1.238 &    0.423 &    0.397 &    0.841 &    0.027 &    0.063 &    0.037 &    1.030 &    0.118 &    0.061 &    0.345\\
\cline{2-16}
 &RF &   0.184 &    0.233 &    0.153 &    1.300 &    0.452 &    0.298 &    0.610 &    0.063 &    0.054 &    0.040 &    1.018 &    0.219 &    0.110 &    0.296\\
\cline{2-16}
 &ANN &   0.218 &    0.174 &    0.172 &    1.574 &    0.571 &    0.367 &    77.616 &    0.124 &    0.104 &    0.103 &    1.695 &    0.407 &    0.210 &    8.119\\
\cline{2-16}
\hline
\end{tabular}
\caption{Forecasting errors measured as symmetric MAPE.}
\label{fg:feature_selection_res}
\end{table*}


\begin{figure}
\centering
%\subfloat[]{\includegraphics[width=0.18\textwidth]{fig/feature_selection/HascData/data_EnergyWeather1001.png}}
\subfloat[]{\includegraphics[trim= 2cm 0 0 15cm,clip,width=0.25\textwidth]{fig/feature_selection/EnergyWeather/data_segms_orig_train.png}}
%\subfloat[]{\includegraphics[trim= 2cm 0 0 15cm,clip,width=0.25\textwidth]{fig/feature_selection/EnergyWeather/res_orig_train.png}}
\caption{(a) Target variables of the design matrix composed of the time series from the  Energy-Weather data set.
 (b) Forecasting results for	Energy-Weather.}\label{fg:target_data}
\end{figure}





%
\begin{figure}
\centering
\subfloat[MLR, Energy]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/EnergyWeather/res_orig_train_target_VAR_fs_pca.png}}
\subfloat[MSVR, Precipitation]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/EnergyWeather/res_orig_train_Precipitation_MSVR_fs_pca.png}}\\
\subfloat[RF, Humidity]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/EnergyWeather/res_orig_train_Relative_Humidity_Random_Forest_fs_NW.png}}
\subfloat[ANN, Solar]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/EnergyWeather/res_orig_train_Solar_Neural_network.png}}\\
\subfloat[RF, Wind]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/EnergyWeather/res_orig_train_Wind_Random_Forest_fs_all.png}}
\subfloat[MLR, Min. T]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/EnergyWeather/res_orig_train_Min_Temperature_VAR_fs_pca.png}}\\

\caption{ Forecasting results for original Energy-Weather data set with all feature generation strategies applied and PCA feature selection.}\label{fg:pca_frc}
\end{figure}


\section{Discussion and conclusion}
In this paper we have suggested a framework for multiscale time series forecast. The proposed framework employs regression-based approach combined with feature generation. We have found that even for such naive approach the results are still better then those of the baseline method. Though the results are somewhat discouraging, we expect further improvement
associated with application of mixtures of experts~\cite{Yuksel2015}, ensembles of weak learners, where each learner is relevant to some subspace of the feature space. For this reason we introduce such feature generation strategies, based on local approximation parameters and distances to centroids: these kinds of features have proven efficient in time series classification problems~\cite{Ignatov2015}.


\nocite{*}
\bibliographystyle{IEEEtran}%{unsrt}
\bibliography{MultiscaleForecasting}


\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{algorithm}[!h]
%%\DontPrintSemicolon
%%\textbf{\emph{ComputeForecastingErrors()}}\\
% \KwData{$\bX^{*} \in \mathbb{R}^{m\times(n+r)}$. Parameters: sample size $m$, train to test ratio $\alpha$, minimal sample (test) size $m_{\min}$.}
% \KwResult{Forecasting error.}
% Decompose matrix $\bX^{*}$ into train and test matrices:
% \[\bX^{*}_{\text{train}}, \; \bX^{*}_{\text{test}} = TrainTestSplit(\bX^{*}, \alpha);\]
% Optimize hyperparameters $\hat{\beta}$ of the model $\fx$, using $\bX^{*}_{\text{train}}$ \;
% \For{$i$ in $\{1, \dots,  m_{\text{test}} - m_{\min}\}$:}{
% define $\bX^{*}_{\text{train}, i}$ as $(i+1)$-th to $(i+ m_{\min} + 1)$-th rows of $\bX^{*}_{\text{test}}$ \;
% define $\bx^{*}_{\text{val}, i}$ as the $i$-th row of $\bX^{*}_{\text{test}}$ \;
%  train forecasting model $\fx(\x, \hat{\w}_i, \hat{\beta})$, using $\bX^{*}_{\text{train}, i}$ and \;
%  obtain vector of residuals $\veps  = \by_{\text{val}, i} - \fx(\bx_{\text{val}, i}, \hat{\w}_i, \hat{\beta})$ \;
%  compute forecasting quality:
%  \[ {SMAPE}(i)  = \frac{1}{r} \sum_{t=1}^{r} \frac{2|\varepsilon_{t}|}{|2(y_{\text{val}, i})_t - \varepsilon_{t}|};\]
%  }
%  Return $SMAPE$, averaged over data splits:
%  \[ \text{Error}  = \frac{1}{m_{\text{test}} - m_{\min}} \sum_{i=1}^{m_{\text{test}} - m_{\min}} {SMAPE}(i).\]
%  \bigskip
% \caption{Compute forecasting errors imitating rolling forecast procedure.}\label{alg:train_test_rmse}
%\end{algorithm}
%
%\begin{algorithm}[!h]
%\DontPrintSemicolon
%\textbf{\emph{TrainTestSplit()}}\\
%\KwData{Object-feature matrix $\bX^{*} \in \mathbb{R}^{m\times(n+r)}$. Train to test ratio $\alpha \in [0, 1]$.}
% \KwResult{Train and test, $\bX^{*}_{\text{test}}$, $\bX^{*}_{\text{val}}$.}
% Set train set and test set sizes:
%
% $ \quad m_{\text{train}} = \lfloor\alpha\cdot m\rfloor, \quad m_{\text{test}} = m - m_{\text{train}} $ \; %(1-\alpha)(m-1)
% Decompose matrix $\bX^{*}$ into train and test matrices $\bX^{*}_{\text{train}}$, $\bX^{*}_{\text{test}}$:
% \[\bX^{*} = \left[\begin{array}{c|c}
% \underset{m_{\text{test}}{\times}n}{\bX_{\text{test}}} & \underset{m_{\text{test}}{\times}r}{\bY_{\text{test}}}  \\
% \hdashline
% \underset{m_{\text{train}}{\times}n}{\bX_{\text{train}}}  & \underset{m_{\text{train}}{\times}r}{\bY_{\text{train}}}
% \end{array}\right]
% \]
%  \caption{Initial train-test split procedure.}\label{alg:train_test_rmse}
%\end{algorithm}


%\begin{figure}
%\centering
%\subfloat[VAR]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_VAR.png}}
%\subfloat[SVR]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_SVR.png}}\\
%\subfloat[Random forest]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_Random_Forest.png}}
%\subfloat[ANN]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_Neural_network.png}}\\
%\caption{Forecasts of HASC1001 with historical features only.}\label{fg:hitorical_data_frc}
%\end{figure}



%\begin{figure}
%\centering
%\subfloat[VAR]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_VAR_fs_Conv.png}}
%\subfloat[SVR]{\includegraphics[trim= 2cm 0 0 15cm,clip,width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_SVR_fs_Conv.png}}\\
%\subfloat[Random forest]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_Random_Forest_fs_Conv.png}}
%\subfloat[ANN]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_Neural_network_fs_Conv.png}}\\
%\caption{Forecasts of HASC1001 with convolutional features added.}
%\end{figure}

%\begin{figure}
%\centering
%\subfloat[SSA]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/generation_HASC1001_fs_SSA.png}}
%\subfloat[SSA]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_fs_SSA.png}}\\
%\subfloat[Cubic]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/generation_HASC1001_fs_Cubic.png}}
%\subfloat[Cubic]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_fs_Cubic.png}} \\
%\subfloat[Conv]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/generation_HASC1001_fs_Conv.png}}
%\subfloat[Conv]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_fs_Conv.png}} \\
%\subfloat[NW]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/generation_HASC1001_fs_NW.png}}
%\subfloat[NW]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_fs_NW.png}} \\
%\subfloat[All]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/generation_HASC1001_fs_all.png}}
%\subfloat[All]{\includegraphics[trim= 2cm 0 0 15cm,clip, width=0.25\textwidth]{fig/feature_selection/HascData/res_HASC1001_fs_all.png}}
%\caption{Forecasting results for various feature generation strategies, HASC1001.}\label{fg:feature_gen_frc}
%\end{figure} 